[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "Introduction\n\nGeospatial (vector) data cookbook using Databricks Spatial SQL with a helping hand from DuckDB Spatial Extension where needed.\nAll pages are available as IPython notebooks as well in the source GitHub repository and they were successfully tested on Databricks Free Edition, Serverless environment version 4 unless otherwise noted.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html",
    "title": "DuckDB on Databricks",
    "section": "",
    "text": "Setting up DuckDB on Databricks\nDuckDB is a formidable new single-machine analytics tool, tracing its origins to the same Dutch research institute as Python. Crucially for this guide, it comes with a remarkably good Spatial extension.\nWhile Databricks comes with its own set of geospatial features, such as ST functions and H3 functions, nothing stops you to use DuckDB on the side as well.\n(What you do have to keep in mind though is that while much of Databricks’s tooling, namely Apache Spark, is focused on big data analysis multi-node clusters, your DuckDB instead will just run on single-node, just like e.g. Pandas would. So use single-node clusters, or Spark UDFs.)\n%pip install duckdb --quiet\n\nimport duckdb\n\n# Install the Spatial Extension:\nduckdb.sql(\"install spatial; load spatial\")\nThis allows you to directly use the DuckDB Spatial, for example:\nduckdb.sql(\"select st_distance(st_point(3, 0), st_point(0, 4)) d\")\n\n# Returns:\n\n# ┌────────┐\n# │   d    │\n# │ double │\n# ├────────┤\n# │    5.0 │\n# └────────┘",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#setting-up-duckdb-on-databricks",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#setting-up-duckdb-on-databricks",
    "title": "DuckDB on Databricks",
    "section": "",
    "text": "Note\n\n\n\nIf install spatial fails (especially if you are not using the Free Edition or Serverless Compute, but classic compute), check whether HTTP is blocked on your (corporate) network. If so, then you need to work around it as described here.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#visualize-duckdb-spatial-output",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#visualize-duckdb-spatial-output",
    "title": "DuckDB on Databricks",
    "section": "Visualize DuckDB Spatial output",
    "text": "Visualize DuckDB Spatial output\nIf your data is simply lon/lat points, you can make use of the built-in point map visualization in Databricks Notebooks if you convert the DuckDB to a Spark DataFrame via Pandas.\nFollowing the New York City pizza restaurants example, but let’s switch to Amsterdam:\n\nquery = duckdb.sql(\n    \"\"\"\nwith t as (\n  SELECT\n    id,\n    names.primary as name,\n    confidence AS confidence,\n    CAST(socials AS JSON) as socials,\n    geometry\n  FROM\n    read_parquet('s3://overturemaps-us-west-2/release/2025-07-23.0/theme=places/type=place/*')\n  WHERE\n    categories.primary = 'pizza_restaurant'\n    AND bbox.xmin BETWEEN 4.7 AND 5.0\n    AND bbox.ymin BETWEEN 52.3 AND 52.4\n)\nselect st_x(geometry) lon, st_y(geometry) lat, name from t\n\"\"\"\n)\nquery\n\n# Returns:\n\n# ┌───────────┬────────────┬──────────────────────────────────────────┐\n# │    lon    │    lat     │                   name                   │\n# │  double   │   double   │                 varchar                  │\n# ├───────────┼────────────┼──────────────────────────────────────────┤\n# │  4.762994 │ 52.3099144 │ Per Tutti                                │\n# │ 4.7789755 │ 52.3381557 │ New York Pizza                           │\n# │ 4.7811585 │ 52.3367951 │ CiCi Pizza                               │\n# │     ·     │      ·     │       ·                                  │\n# │     ·     │      ·     │       ·                                  │\n# │     ·     │      ·     │       ·                                  │\n\nOnce the result below is shown, click on the + icon right of the Table tab to add the visualization “Map (Markers)” such as the one shown on the below image.\n\nspark.createDataFrame(query.df()).display()\n\n\n\n\npoint_map\n\n\nOr visualize with lonboard, which will work also for other geometry types like linestrings and polygons, again following an Overture Maps example:\n\n%pip install lonboard shapely --quiet\n\nfrom lonboard import viz\n\n\nquery = duckdb.sql(\n    \"\"\"\n  SELECT\n    subtype,\n    names.primary as name,\n    geometry\n  FROM\n    read_parquet('s3://overturemaps-us-west-2/release/2025-07-23.0/theme=divisions/type=division_area/*')\n  WHERE\n    4.7 &lt; bbox.xmax AND bbox.xmin &lt; 5.0\n    AND 52.3 &lt; bbox.ymax AND bbox.ymin &lt; 52.4\n    AND subtype = 'county'\n    AND country = 'NL'\n\"\"\"\n)\n\nviz(query).as_html()\n\n\n\n\nlonboard_gemeente_map\n\n\nNote that clicking on a polygon opens a table with its parameters, in this case the municipalities of Amsterdam and some of its neighbors.\nAs powerful as Lonboard is, it won’t be able to visualize extremely large numbers of geometries, so if you try and fail at a larger example, try filtering your objects further.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#write-delta-lake-tables-from-duckdb",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#write-delta-lake-tables-from-duckdb",
    "title": "DuckDB on Databricks",
    "section": "Write Delta Lake Tables from DuckDB",
    "text": "Write Delta Lake Tables from DuckDB\nIf you want to write a result to a delta lake table (or temporary view), you can use Pandas as an intermediary format:\n\nspark.createDataFrame(query.df()).createOrReplaceTempView(\"t\")\n\n# Or write a persistent table, instead of a temporary view:\n# spark.createDataFrame(query.df()).write.saveAsTable(\"t\")\n\n\n%sql\nselect\n  name\nfrom\n  t\n-- Returns:\n-- ┌────────────────┐\n-- │      name      │\n-- ├────────────────┤\n-- │ Haarlemmermeer │\n-- │ Aalsmeer       │\n-- │ De Ronde Venen │\n-- │ Amstelveen     │\n-- │ Ouder-Amstel   │\n-- │ Amsterdam      │\n-- │ Diemen         │\n-- │ Waterland      │\n-- └────────────────┘",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#read-delta-lake-tables-with-duckdb",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#read-delta-lake-tables-with-duckdb",
    "title": "DuckDB on Databricks",
    "section": "Read Delta Lake Tables with DuckDB",
    "text": "Read Delta Lake Tables with DuckDB\nWe can read moderate amount of data from a delta table to duckdb via Arrow (or Pandas). (This assumes that the data volume is not prohibitively large to load into the memory of a single machine.)\n\ndfa = spark.read.table(\"t\").toArrow()\n\nquery = duckdb.sql(\n    \"\"\"\nselect\n    name\nfrom\n    dfa;\n\"\"\"\n)\nquery\n\n# Returns:\n\n# ┌────────────────┐\n# │      name      │\n# │    varchar     │\n# ├────────────────┤\n# │ Haarlemmermeer │\n# │ Aalsmeer       │\n# │ De Ronde Venen │\n# │ Amstelveen     │\n# │ Ouder-Amstel   │\n# │ Amsterdam      │\n# │ Diemen         │\n# │ Waterland      │\n# └────────────────┘\n\nAnother, more scalable way to read Delta Lake tables with DuckDB is to use the Databricks Temporary Table Credentials API (not available on Databricks Free Edition as of writing) and the DuckDB Delta extension.\nFinally, a high-level tool to make this work would be the DuckDB extension uc-catalog, but this did not reliably work for me (on either Free Edition or “paid” Databricks) as of writing.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#use-cell-magic-jupysql-for-duckdb-queries",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#use-cell-magic-jupysql-for-duckdb-queries",
    "title": "DuckDB on Databricks",
    "section": "Use cell magic %%jupysql for duckdb queries",
    "text": "Use cell magic %%jupysql for duckdb queries\nIn Databricks, you can write Databricks SQL code (so not DuckDB SQL code) in SQL code cells, using the %sql cell magic. DuckDB is also compatible with similar cell magic tooling, but in order to not get crossed with Databricks SQL, we’ll need to use another magic, %%jupysql (which is also the name of the package that makes this possible). We can set it all up as the following:\n\n%pip install jupysql --quiet\n%restart_python\n\n\n%load_ext sql\n\n\nimport duckdb\n\ncon = duckdb.connect()\n\nWe need to specify the connection object only at the first use of %%jupysql, as below:\n\n%%jupysql con --alias duckdb\ninstall spatial; load spatial\n\n\n%%jupysql\nwith t as (\n  SELECT\n    id,\n    names.primary as name,\n    confidence AS confidence,\n    CAST(socials AS JSON) as socials,\n    geometry\n  FROM\n    read_parquet('s3://overturemaps-us-west-2/release/2025-07-23.0/theme=places/type=place/*')\n  WHERE\n    categories.primary = 'pizza_restaurant'\n    AND bbox.xmin BETWEEN 4.7 AND 5.0\n    AND bbox.ymin BETWEEN 52.3 AND 52.4\n)\nselect st_x(geometry) lon, st_y(geometry) lat, name from t\n\n-- Returns:\n\n-- Running query in 'duckdb'\n-- lon  lat name\n-- 4.762994 52.3099144  Per Tutti\n-- 4.7789755    52.3381557  New York Pizza\n-- 4.7811585    52.3367951  CiCi Pizza\n-- 4.7812061    52.3368685  Joey's kitchen\n-- 4.7500226    52.3816196  Gastronoom\n-- 4.7905345    52.34096    Amon\n-- 4.7948139    52.3512209  Moeruth Pizza\n-- 4.797318 52.3516662  Il Delfino Blu\n-- 4.7988733    52.351978   Domino's Pizza\n-- 4.8107487    52.3539346  New York Pizza\n-- Truncated to displaylimit of 10.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#register-duckdb-functions-as-spark-udfs",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#register-duckdb-functions-as-spark-udfs",
    "title": "DuckDB on Databricks",
    "section": "Register DuckDB functions as Spark UDFs",
    "text": "Register DuckDB functions as Spark UDFs\nSee here.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "delta/GEOMETRY_vs_GEOGRAPHY.html",
    "href": "delta/GEOMETRY_vs_GEOGRAPHY.html",
    "title": "GEOMETRY vs GEOGRAPHY vs WKB",
    "section": "",
    "text": "If in a Delta Lake table you only store (lat, lon) values, use GEOGRAPHY (with the caveat that for st_distanceshpere and st_distancespheroid to work, you would need to cast geographies to geometry as geog::geometry(4326)).\nIf you use an SRID other than 4326 or multiple SRID’s, use GEOMETRY. If you need to calculate values in meters from lat/lon, you can still cast to GEOGRAPHY as needed and use measurement functions like st_area, st_length or st_perimeter.\nIf you need compatility with other tools, you might consider WKB – however, spark.write.parquet() would export GEOMETRY/GEOGRAPHY columns as WKB anyway.",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GEOMETRY vs GEOGRAPHY vs WKB</span>"
    ]
  },
  {
    "objectID": "delta/GEOMETRY.html",
    "href": "delta/GEOMETRY.html",
    "title": "Storing spatial data in Delta Lake as GEOMETRY type",
    "section": "",
    "text": "Cleanup\nYou can store geometry or geography data in a Delta Lake table in a GEOMETRY column. See also GEOMETRY_vs_GEOGRAPHY.\nNote the geometry(4326) type above.\n(Note that you could mix multiple SRID’s in one GEOMETRY column inside a Spark Dataframe, including inside a view, but must have a single SRID in a table.)\nLet’s calculate the distance in kilometers (on the surface of the spheroid Earth) between the two point examples above. st_distancespheroid expects lat/lon, so we’ll first convert everything to SRID 4326 – Null Island already is though.\nThe GEOGRAPHY type also offers further functions that take lon/lat as input and produce meters as output, such as st_area. So let’s calculate the size of the Bermuda Triangle in square kms:\n%sql\n -- drop table tmp_geometries",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Storing spatial data in Delta Lake as GEOMETRY type</span>"
    ]
  },
  {
    "objectID": "delta/GEOGRAPHY.html",
    "href": "delta/GEOGRAPHY.html",
    "title": "Storing spatial data in Delta Lake as GEOGRAPHY type",
    "section": "",
    "text": "Cleanup\nYou can store geography data in a Delta Lake table in a GEOGRAPHY column. See also GEOMETRY_vs_GEOGRAPHY.\nNote the data type geography(4326) above.\nLet’s calculate the distance in kilometers (on the surface of the spheroid Earth) between the two point examples above. st_distancespheroid expects GEOMETRIES of lat/lon, so we’ll need to cast.\nThe GEOGRAPHY type also offers further functions that take lon/lat as input and produce meters as output, such as st_area. So let’s calculate the size of the Bermuda Triangle in square kms:\n%sql\n -- drop table tmp_geographies",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Storing spatial data in Delta Lake as GEOGRAPHY type</span>"
    ]
  },
  {
    "objectID": "delta/WKB.html",
    "href": "delta/WKB.html",
    "title": "Storing spatial data in Delta Lake as WKB",
    "section": "",
    "text": "Example usage with ST functions\nYou can store geometry or geography data in a Delta Lake table in a BINARY column as Well-known binary (WKB or EWKB). This is a more compact representation than well-known text (WKT), and widely supported incl. in the Geoparquet specification. On the other hand, unlike the newer GEOMETRY and GEOGRAPHY types, there is no higher level semantic support possible. Also, you need to use the conversion function st_geomfromwkb or st_geomfromwekb before any other ST function.\nAnother example of Delta Lake tables with WKB columns are the Overture Maps datasets prepared by CARTO, available via the Databricks Marketplace. Follow the previous link to add any them (at no cost) to your catalog, if you haven’t yet. For example, for the below query, use Divisions (borders of countries and other administrative divisions):\nThese CARTO tables also show one pattern to organize and cluster tables with geometries: they include the bounding box columns __carto_xmin, __carto_xmax, __carto_ymin, ___carto_ymax and are clustered by these colums.\nAnother pattern would be to make use of spatial indexing such as H3.\n%sql\nwith countries as (\n  select\n    country,\n    st_geogfromwkb(geometry) geography\n  from\n    carto_overture_maps_divisions.carto.division_area\n  where\n    subtype = 'country'\n    and class = 'land'\n    and country in ('GB', 'FR')\n)\nselect\n  country,\n  st_area(geography) / 1e6 area_km2s\nfrom\n  countries\n-- Returns:\n-- country  area_km2s\n-- FR   549231.6644010496\n-- GB   244408.1099778328",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Storing spatial data in Delta Lake as WKB</span>"
    ]
  },
  {
    "objectID": "other_formats/import.html",
    "href": "other_formats/import.html",
    "title": "Import geospatial formats into Delta Lake with DuckDB",
    "section": "",
    "text": "Setup\nThis example focuses on a few example formats, but the same workflow works just as much for any spatial format that DuckDB Spatial supports via its GDAL integration, see the output of ST_Drivers.\n%pip install duckdb --quiet\nimport duckdb\n\nduckdb.sql(\"install spatial; load spatial\")\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\n\nspark.sql(f\"use {CATALOG}.{VOLUME}\")",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import geospatial formats into Delta Lake with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/import.html#setup",
    "href": "other_formats/import.html#setup",
    "title": "Import geospatial formats into Delta Lake with DuckDB",
    "section": "",
    "text": "Note\n\n\n\nIf install spatial fails (especially if you are not using the Free Edition or Serverless Compute, but classic compute), check whether HTTP is blocked on your (corporate) network. If so, then you need to work around it as described here.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import geospatial formats into Delta Lake with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/import.html#import-geopackage",
    "href": "other_formats/import.html#import-geopackage",
    "title": "Import geospatial formats into Delta Lake with DuckDB",
    "section": "Import Geopackage",
    "text": "Import Geopackage\n\n\n\n\n\n\nNote\n\n\n\nNOTE: This example is focusing on using DuckDB to parse the GeoPackage. For more complex GeoPackages, you may need to install GDAL and use the GDAL command line tools.\n\n\n\nGPKG_URL = \"https://service.pdok.nl/kadaster/bestuurlijkegebieden/atom/v1_0/downloads/BestuurlijkeGebieden_2025.gpkg\"\n\n\nlayers = duckdb.sql(\n    f\"\"\"\nwith t as (\n    select unnest(layers) layer\n     from st_read_meta('{GPKG_URL}'))\nselect\n    layer.name layer_name,\n    layer.geometry_fields[1].name geom_field\nfrom t\"\"\"\n).df()\n\nlayers\n\n# Returns:\n\n# layer_name    geom_field\n# 0 gemeentegebied  geom\n# 1 landgebied  geom\n# 2 provinciegebied geom\n\n\n# pick a layer to read\nlayer_name, geom_field = layers.loc[0, [\"layer_name\", \"geom_field\"]]\n\nduckdb.sql(\n    f\"\"\"copy (\n  select * replace(st_aswkb({geom_field}) as {geom_field})\n  from\n    st_read(\n      '{GPKG_URL}',\n      layer='{layer_name}')\n  ) to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{layer_name}.parquet' (format parquet)\"\"\"\n)\n\n\nspark.read.parquet(\n    f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{layer_name}.parquet\"\n).display()\n\nYou can store the above spark data frame as a Delta Lake table as needed:\n\n# TABLENAME = None  # FILL ME IN\n\n# spark.read.parquet(\n#     f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{layer_name}.parquet\"\n# ).write.saveAsTable(f\"{CATALOG}.{SCHEMA}.{TABLENAME}\")",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import geospatial formats into Delta Lake with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/import.html#import-openstreetmap-data",
    "href": "other_formats/import.html#import-openstreetmap-data",
    "title": "Import geospatial formats into Delta Lake with DuckDB",
    "section": "Import OpenStreetMap data",
    "text": "Import OpenStreetMap data\nIf you need data from OpenStreetMap (OSM) that is also available via Overture Maps, you are way better off using the latter. You could follow their DuckDB tutorial, or, even better, make use of CARTO’s pre-loaded delta lake tables via the Marketplace.\nHowever, by far not all OSM data is available in Overture Maps. For example, transit data is absent. If you need such data layers, you’ll need to load OSM data yourself, such as below.\nPick your desired area to download at https://download.geofabrik.de/ , or, not really recommended, but you could try loading the whole world via https://planet.openstreetmap.org/ .\n\nGEOFABRIK_URL = \"https://download.geofabrik.de/europe/netherlands-latest.osm.pbf\"\n\n\nfile_name = GEOFABRIK_URL.split(\"/\")[-1]\nfile_basename = file_name.rsplit(\".\")[0]\nvolume_file_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{file_name}\"\nvolume_parquet_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{file_basename}.parquet\"\n\n\n!curl -o {volume_file_path} {GEOFABRIK_URL}\n\nThe below humble script actually does quite some heavy lifting: DuckDB Spatial recognizes the .osm.pbf file as an OSM extract, and calls ST_Read_OSM under the hood.\n\nduckdb.sql(\n    f\"\"\"\ncopy (\n    select\n        *\n    from\n        '{volume_file_path}'\n) to '{volume_parquet_path}'\n(format parquet)\n;\n\"\"\"\n)\n\nYou can further process this dataset into Nodes, Ways, and Relations with SQL, which is beyond the scope of this doc, but nevertheless here is a minimal example to visualize some data (showing the route shapes of the Dutch train class Intercity Direct):\n\n# Calling `clusterBy` below will take some [time](url), but as it takes multiple joins\n# by `id` to build any structure in OSM, it is worth it.\nspark.read.parquet(volume_parquet_path).write.clusterBy(\"id\").saveAsTable(\"tmp_osm\")\n\n\n%sql\ncreate or replace table tmp_route_shapes as\nwith route as (\n  select\n    id as route_id,\n    explode(refs) as id\n  from\n    tmp_osm\n  where\n    tmp_osm.tags.type = 'route'\n    and tmp_osm.tags.route = 'train'\n    and lower(tmp_osm.tags.brand) = 'intercity direct'\n),\nrail as (\n  select\n    id as rail_id,\n    posexplode(refs) as (pos, id)\n  from\n    route join tmp_osm using (id)\n  where\n    tags.railway = 'rail'\n)\nselect\n  rail_id,\n  st_makeline(\n    collect_list(st_point(tmp_osm.lon, tmp_osm.lat)) over (partition by rail_id order by pos)\n  ) as geometry\nfrom\n  rail join tmp_osm using (id)\n\n\ndef spark_viz(df, wkb_col=\"geometry\", other_cols=None, limit=10_000, output_html=None):\n    # needs `%pip install duckdb lonboard shapely`\n\n    if other_cols is None:\n        other_cols = []\n\n    import duckdb\n    from lonboard import viz\n\n    try:\n        duckdb.load_extension(\"spatial\")\n    except duckdb.duckdb.IOException:\n        duckdb.install_extension(\"spatial\")\n        duckdb.load_extension(\"spatial\")\n\n    dfa = df.select([wkb_col] + other_cols).limit(limit).toArrow()\n    if dfa.num_rows == limit:\n        print(f\"Data truncated to limit {limit}\")\n\n    query = duckdb.sql(\n        f\"\"\"select * replace (st_geomfromwkb({wkb_col}) as {wkb_col})\n        from dfa\n        where {wkb_col} is not null\"\"\"\n    )\n    if output_html is None:\n        return viz(query).as_html()\n    else:\n        viz(query).to_html(output_html)\n        return output_html\n\n\nspark_viz(\n    spark.table(\"tmp_route_shapes\").selectExpr(\"st_asbinary(geometry) as geometry\"),\n    limit=100_000,\n)\n\n\n\n\nicd",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import geospatial formats into Delta Lake with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/import.html#cleanup",
    "href": "other_formats/import.html#cleanup",
    "title": "Import geospatial formats into Delta Lake with DuckDB",
    "section": "Cleanup",
    "text": "Cleanup\n\n%sql\n -- drop table tmp_nodes;\n -- drop table tmp_route_shapes;",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import geospatial formats into Delta Lake with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html",
    "href": "other_formats/export.html",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "",
    "text": "Setup\n%pip install \"databricks-connect&gt;=17.1.0\" duckdb --quiet\n%restart_python\nimport duckdb\n\nduckdb.sql(\"install spatial; load spatial\")\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\n\nGEOMETRY_COLUMN = \"geometry\"\n\nspark.sql(f\"create volume if not exists {CATALOG}.{SCHEMA}.{VOLUME}\")\nLet’s first create an example table with GEOMETRY columns:\n%sql\ncreate or replace table tmp_geometries as\nselect\n  st_point(0, 0, 4326) as geometry,\n  \"Null Island\" as name\nunion all\nselect\n  st_transform(st_point(155000, 463000, 28992), 4326) as geometry,\n  \"Onze Lieve Vrouwetoren\" as name\nunion all\nselect\n  st_makepolygon(\n    st_makeline(\n      array(\n        st_point(- 80.1935973, 25.7741566, 4326),\n        st_point(- 64.7563086, 32.3040273, 4326),\n        st_point(- 66.1166669, 18.4653003, 4326),\n        st_point(- 80.1935973, 25.7741566, 4326)\n      )\n    )\n  ) as geometry,\n  \"Bermuda Triangle\" as name;\n\nfrom\n  tmp_geometries\n-- Returns:\n-- geometry name\n-- SRID=4326;POINT(0 0) Null Island\n-- SRID=4326;POINT(5.3872035084137675 52.15517230119224)    Onze Lieve Vrouwetoren\n-- SRID=4326;POLYGON((-80.1935973 25.7741566,-64.7563086 32.3040273,-66.1166669 18.4653003,-80.1935973 25.7741566)) Bermuda Triangle",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#setup",
    "href": "other_formats/export.html#setup",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "",
    "text": "Note\n\n\n\nIf install spatial fails (especially if you are not using the Free Edition or Serverless Compute, but classic compute), check whether HTTP is blocked on your (corporate) network. If so, then you need to work around it as described here.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure you are using Serverless environment version 4+, or else outputting GEOMETRY/GEOGRAPHY types directly (without e.g. st_asewkt()) will not work.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#parquet-files",
    "href": "other_formats/export.html#parquet-files",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "Parquet files",
    "text": "Parquet files\nWe’ll use DuckDB Spatial to write he Geoparquet file, so first, we output the above Delta Lake table as a directory of Parquet files, using lon/lat coordinates.\n(You could also use Databricks Temporary Table Credentials API to directly read the Delta Lake table with the DuckDB Delta Extension instead.)\n\nfrom pyspark.sql import functions as F\n\nspark.table(\"tmp_geometries\").withColumn(\n    \"geometry\", F.expr(\"st_transform(geometry, 4326)\")\n).write.mode(\"overwrite\").parquet(\n    f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet\"\n)\n\nWe will use the above parquet export as a stepping stone to produce other formats below.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#geoparquet",
    "href": "other_formats/export.html#geoparquet",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "Geoparquet",
    "text": "Geoparquet\nWe can use duckdb to transform the Parquet files into a valid Geoparquet files:\n\n\n\n\n\n\nNote\n\n\n\nIf you didn’t load the DuckDB Spatial extension, the below would still succeed but Geoparquet metadata would not be written.\n\n\n\nquery = f\"\"\"\nload spatial;\ncopy (\nselect \n    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\nfrom\n    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n) to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries_geo.parquet' (format parquet)\"\"\"\nduckdb.sql(query)\n\nThere are more details around writing Geoparquet such as writing custom CRS’s or defining a “covering” using bounding boxes, but the above example is already a valid Geoparquet. For example, if your QGIS already supports the Parquet format (as of Aug 2025, the latest Windows version does but the latest macOS version doesn’t), then you can open this file in QGIS (after having downloaded from Volumes):\n\n\n\ngeoparquet in qgis\n\n\n(in fact, the GDAL Parquet reader used by QGIS can even open parquet files that are not valid geoparquet, as long as they have a WKB or WKT column and the column name and CRS matches the expected defaults or correctly defined)",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#thumbnail-sampled",
    "href": "other_formats/export.html#thumbnail-sampled",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "Thumbnail (sampled)",
    "text": "Thumbnail (sampled)\nWe can create a thumbnail (useful e.g. for STAC) as follows. This approach will only work for small datasets (as it’s loaded into memory), so for larger datasets, you’d need to write out a sampled Geoparquet first – or use another technique that is not sampled (see PMTiles below for an un-sampled visualization format).\n\n%pip install geopandas pandas==$(pip show pandas | awk '/^Version:/ {print $2}') --quiet\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\ngdf = gpd.read_parquet(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries_geo.parquet\")\n\nfig, ax = plt.subplots(figsize=(4, 4))\ngdf.plot(ax=ax, markersize=2)\nplt.axis(\"off\")",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#flatgeobuf",
    "href": "other_formats/export.html#flatgeobuf",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "Flatgeobuf",
    "text": "Flatgeobuf\nExporting to Flatgeobuf is very similar to the above. Flatgeobuf as a format has two key advantages here: - It is faster to render (e.g. in QGIS) than Geoparquet, and - It can act as input to tippecanoe (see below), which we’ll use to produce PMTiles, which is even better suited for web mapping.\n\nquery = f\"\"\"\nload spatial;\ncopy (\nselect \n    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\nfrom\n    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n) to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb'\n(FORMAT GDAL, DRIVER flatgeobuf, LAYER_CREATION_OPTIONS 'TEMPORARY_DIR=/tmp/')\"\"\"\nduckdb.sql(query)\n\n\n\n\n\n\n\nCaution\n\n\n\nIf your table contains STRUCTs, the GDAL driver won’t like them. You’d need to drop them or move the the sub-fields up with the “.” operator, such as select * except (struct_col), struct_col.*.\nFurthermore, the FlatGeobuf writer is overeager with BINARY/BLOB columns: if you have any BINARY/BLOB columns besides your chosen geometry column, they will be also parsed as WKB, possibly with unexpected geometry data types, which will cause issues downstream when trying to convert to PMTiles with tippecanoe. Long story short, additionaly BINARY/BLOB columns beyond your geometry column need to be filtered out as well.\n\n\n\nStreaming the Flatgeobuf file to QGIS\nYou can stream this to QGIS (i.e. without downloading the file first – this is very useful for much larger datasets) via a token and the Files API. For example, after setting up a personal access token, you can stream the above file with a link like below:\n\nf\"/vsicurl?header.Authorization=Bearer%20&lt;YOUR_PERSONAL_ACCESS_TOKEN&gt;&url=https://{spark.conf.get('spark.databricks.workspaceUrl')}/api/2.0/fs/files/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb\"\n\nReplace &lt;YOUR_PERSONAL_ACCESS_TOPEN&gt; in the output of the above with your token, and you can copy the resulting string (together with “/vsicurl” at the beginning, but without the quotes) to QGIS, inserting a vector layer.\n\n\n\n\n\n\nNote\n\n\n\nThis way of streaming might work with other formats too, such as Parquet; however, for larger datasets, Flatgeobuf is a great choice. And for smaller datasets, simply downloading the file might be faster than setting up the above authentication.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#pmtiles",
    "href": "other_formats/export.html#pmtiles",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "PMTiles",
    "text": "PMTiles\nFor PMTiles, while theoretically we could keep using DuckDB Spatial with GDAL, it did not succeed in our testing, so we’ll instead use tippecanoe.\nThe below make install runs in 3-4 min on high memory serverless.\n\n%sh\n# ~3.5 min on high memory serverless https://docs.databricks.com/aws/en/compute/serverless/dependencies#high-memory\ngit clone https://github.com/felt/tippecanoe.git\ncd tippecanoe\nmake -j\nmake install PREFIX=$HOME/.local\nrm -r tippecanoe\n\n\nimport os\n\nHOME = os.environ[\"HOME\"]\n\n# see https://github.com/felt/tippecanoe/blob/main/README.md#try-this-first and e.g.\n# https://github.com/OvertureMaps/overture-tiles/blob/main/scripts/2024-07-22/places.sh\n# for possible options\n!{HOME}/.local/bin/tippecanoe -zg -rg -o /tmp/geometries.pmtiles  --simplification=10 --drop-smallest-as-needed --drop-densest-as-needed --extend-zooms-if-still-dropping --maximum-tile-bytes=2500000 --progress-interval=10 -l geometries --force /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb\n# NOTE: this mv will emit an error related to updating metadata (\"mv: preserving\n# permissions for ‘[...]’: Operation not permitted\"), this can be ignored.\n!mv /tmp/geometries.pmtiles /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.pmtiles\n\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, you can feed a directory of JSONL to tippecanoe (make sure you use the --read-parallel flag), such as:\n(\n    spark.table('&lt;TABLENAME&gt;')\n    .selectExpr(\n        \"'Feature' as type\",\n        \"parse_json('{}') as properties\",\n        \"parse_json(st_asgeojson(geometry)) as geometry\",\n    )\n    .write.mode(\"overwrite\")\n    .option(\"ignoreNullFields\", \"false\")\n    .json(\"/Volumes/.../output.jsonl\")\n)\nand then read as:\ntippecanoe [...] --read-parallel /Volumes/.../output.jsonl/part-*\nCan come in handy if for some reason the Flatgeobuf export is not working.\n(Yet another approach is to stream JSON with DuckDB Spatial to tippecanoe, as in here, but then tippecanoe cannot parallel read.)\n\n\nTo visualize, download the PMTiles from Volumes, and upload it to https://pmtiles.io/ (see below screenshot).\n\n\n\npmtiles_io\n\n\nOr more elegantly, to directly visualize it via Databricks Apps instead of downloading, you can port pmtiles.io to open the .pmtiles files in Volumes via the Files API – see this app for a rudimentary example with App Authorization.\nTO be clear: the advantage the PMTiles format is to be able to visualize very large datasets such as all of OpenStreetMap – this notebook only uses a very small example.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#geopackage",
    "href": "other_formats/export.html#geopackage",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "GeoPackage",
    "text": "GeoPackage\nAh oldie but goodie! Similar to the FlatGeobuf DuckDB writes abov, but as the writer both needs random access and it doesn’t take an option similar to FlatGeobuf’s TEMPORARY_DIR, you need to first write to /tmp and then move to Volumes:\n\nquery = f\"\"\"\nload spatial;\ncopy (\nselect \n    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\nfrom\n    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n) to '/tmp/geometries.gpkg'\n(FORMAT GDAL, DRIVER GPKG)\"\"\"\nduckdb.sql(query)\n!mv \"/tmp/geometries.gpkg\" \"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.gpkg\"\n\n\nThe GPKG writer might not like all non-geo data types either, for example a varchar[] array would not be supported.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#shapefiles",
    "href": "other_formats/export.html#shapefiles",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "Shapefiles",
    "text": "Shapefiles\nDon’t use shapefiles.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#cleanup",
    "href": "other_formats/export.html#cleanup",
    "title": "Export Delta Lake table to other formats with DuckDB",
    "section": "Cleanup",
    "text": "Cleanup\n\n%sql\n -- drop table tmp_geometries",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "viz/native.html",
    "href": "viz/native.html",
    "title": "Native Point Map Visualization",
    "section": "",
    "text": "Point map visualization is natively supported, if your table outputs longitude-latitude pairs. It is recommended to limit the query to not too many rows for performance reasons.\n\n\n\n\n\n\nNote\n\n\n\nMake sure you are using Serverless environment version 4+, or else outputting GEOMETRY/GEOGRAPHY types directly (without e.g. st_asewkt()) will not work.\n\n\n\n%pip install \"databricks-connect&gt;=17.1.0\" --quiet\n%restart_python\n\n\n%sql\ncreate or replace table tmp_geometries as\nselect\n  \"Onze Lieve Vrouwetoren\" as name,\n  st_transform(st_point(155000, 463000, 28992), 4326) as geometry\nunion all\nselect\n  \"Bermuda Triangle\" as name,\n  st_makepolygon(\n    st_makeline(\n      array(\n        st_point(- 80.1935973, 25.7741566, 4326),\n        st_point(- 64.7563086, 32.3040273, 4326),\n        st_point(- 66.1166669, 18.4653003, 4326),\n        st_point(- 80.1935973, 25.7741566, 4326)\n      )\n    )\n  ) as geometry;\n\nfrom\n  tmp_geometries\n-- Returns:\n-- name geometry\n-- Bermuda Triangle SRID=4326;POLYGON((-80.1935973 25.7741566,-64.7563086 32.3040273,-66.1166669 18.4653003,-80.1935973 25.7741566))\n-- Onze Lieve Vrouwetoren   SRID=4326;POINT(5.3872035084137675 52.15517230119224)\n\nOur example table contains not only points, but the native visualization can only show points, so one thing we can do is to show the centroids.\nIn the above example, the SRID happens to be 4326, therefore the below st_transform is a unnecessary. In general, however, you’d want to transform the geometries to 4326 in order to obtain valid (lon, lat) tuples.\n\n%sql\nwith t as (\n  select\n    name,\n    st_transform(st_centroid(geometry), 4326) centroid,\n    st_x(centroid) lon,\n    st_y(centroid) lat\n  from\n    tmp_geometries\n)\nfrom\n  t\n\nOnce the result above is shown, click on the + icon right of the Table tab to add the visualization “Map (Markers)” such as the one shown on the below image.\n\n\n\nnative_pointmap\n\n\nFor another, larger example of native map visualization, see the Overture example used in the DuckDB notebook.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Native Point Map Visualization</span>"
    ]
  },
  {
    "objectID": "viz/maplibregljs.html",
    "href": "viz/maplibregljs.html",
    "title": "Scalable visualization of large Delta Lake tables with GEOMETRY columms with DuckDB Spatial MVT and MaplibreGL JS",
    "section": "",
    "text": "Create a sample table\nWe have MVT support in DuckDB Spatial since version 1.4. This means that we can feed MVT to e.g. MaplibreGL JS, as shown by DuckDB Spatial author Max Gabrielsson here in an example Flask app.\n(Another nice tool to consume DuckDB MVT’s would be Martin, this is tracked in this issue.)\nBut how do we efficiently generate MVT’s from a Delta Lake table containing a GEOMETRY, given the tile indices?\nThe key thing to consider is that now Databricks has very efficient spatial join filtering via e.g. ST_Intersect, especially if what you are filtering for is a constant. So the following query can be sub-second for e.g. a billion polygons such as Overture Maps buildings (note that we are not using any spatial grid or bounding box filters anymore):\nWe create here a sample table of buildings in the Netherlands – the same worked for me also for all 2.5B Overture Maps buildings of the world, but if you tried to persist that table, you’d probably run against the daily usage limit of Databricks Free Edition as I did.\nOVERTUREMAPS_RELEASE = \"2025-10-22.0\"\nCOUNTRY_CODE = \"NL\"\n\ncountry_bbox = (\n    spark.read.parquet(\n        f\"s3://overturemaps-us-west-2/release/{OVERTUREMAPS_RELEASE}/theme=divisions/type=division_area\"\n    )\n    .where(f\"subtype = 'country' and class = 'land' and country = '{COUNTRY_CODE}'\")\n    .select(\"bbox.*\")\n    .toPandas()\n    .iloc[0]\n)\nfrom pyspark.sql import functions as F\n\nspark.read.parquet(\n    f\"s3://overturemaps-us-west-2/release/{OVERTUREMAPS_RELEASE}/theme=buildings/type=building\"\n).where(\n    f\"\"\"bbox.xmin &lt; {country_bbox[\"xmax\"]}\n        and bbox.xmax &gt; {country_bbox[\"xmin\"]}\n        and bbox.ymin &lt; {country_bbox[\"ymax\"]}\n        and bbox.ymax &gt; {country_bbox[\"ymin\"]}\n        \"\"\"\n).withColumn(\"geometry\", F.expr(\"st_geomfromwkb(geometry)\")).write.mode(\n    \"overwrite\"\n).saveAsTable(\"workspace.default.building_geom\")\nNow we can build on Maxxen’s gist, with the following adjustments:\nIn the below video (showing a table with all 2.5B buildings worldwide, not just one country), you can see the tiling at work – note how 1) the graceful feature limit means that “too busy” tiles are just shown as rectangles, and 2) zoom-and-pan pauses the feature layer but after a short timeout, the tiles are drawn, with sub-second latency per tile.\nFind the full code here, which you can run locally as a Flask app (you could also embed it within a Databricks App if preferred, but the local app is of course a bit more cost-effective).\n(click on the above image to play the video.)",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Scalable visualization of large Delta Lake tables with GEOMETRY columms with DuckDB Spatial MVT and MaplibreGL JS</span>"
    ]
  },
  {
    "objectID": "viz/maplibregljs.html#create-a-sample-table",
    "href": "viz/maplibregljs.html#create-a-sample-table",
    "title": "Scalable visualization of large Delta Lake tables with GEOMETRY columms with DuckDB Spatial MVT and MaplibreGL JS",
    "section": "",
    "text": "We keep DuckDB doing the MVT generation incl. the preprocessing of calculating the ST_TileEnvelope for the tiles needed for the current viewport, but of course we need Databricks SQL to actually spatial filter our Delta Table (DuckDB delta_scan currently does not read GEOMETRY data types.)\n\nAn alternative approach could be to wrap the used DuckDB functions into Spark UDF’s, if we wanted to move some compute from your browser to DBSQL.\n\nFor DBSQL we use the Python databricks-sql-connector, authenticating with a Personal Access Token – for serious work, you’d want to use OAuth instead.\nGraceful feature limit. What to do if a tile has too many features? A common solution would be to define a minimum zoom level, but this would make it very cumbersome to move around the map, so we define a MAX_FEATURES_PER_TILE instead. If this is reached, we gracefully fail and only show the tile boundaries – the user would only need to further zoom in to reveal all the features within that viewport. (Of course you can throttle this value as you wish to find a balance between loading time and number of features shown.)\nMVT expects SRID 3857, while our table is probably in another SRID, so we need to use some st_transform there and back.\nTile throttling. we also added JS code under // === Tile throttling logic === to take a 2 second pause starting any zoom and move interaction, in order to avoid overloading the warehouse with tile requests and therefore avoid tile queueing.\n\nNote that in the current implementation this means that during zooming and moving the map, the feature layer is temporarily not visible – this probably could be improved. For example, without tile throttling, the objects would remain visible during zoom/pan, but we would need to wait much longer for the results after a big move.\n\n\n\n\n\n\n\nWatch the video\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat if you find this approach still too “slow”, from the end-user standpoint? And/or, you find it “cheating” that we use MAX_FEATURES_PER_TILE? Then you can use PMTiles. The difference is that with the MVT approach, you directly read the Delta Lake table, and the PMTile you would need to generate which means extra compute and time.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Scalable visualization of large Delta Lake tables with GEOMETRY columms with DuckDB Spatial MVT and MaplibreGL JS</span>"
    ]
  },
  {
    "objectID": "viz/GeoPandas.html",
    "href": "viz/GeoPandas.html",
    "title": "Visualize with GeoPandas",
    "section": "",
    "text": "While you can use GeoPandas as well to visualize geodata, via Matplotlib, we focus here, on dynamic, slippy maps, such as the ones you can create with lonboard or PMTiles. GeoPandas also integrates with Folium, but we prefer Lonboard.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualize with GeoPandas</span>"
    ]
  },
  {
    "objectID": "viz/lonboard.html",
    "href": "viz/lonboard.html",
    "title": "Visualize a geospatial Delta Lake table with Lonboard",
    "section": "",
    "text": "In-notebook widget example\nLonboard is an excellent tool to visualize geospatial data via Arrow or DuckDB. While it cannot visualize as large datasets as you can with using streaming Flatgeobuf or PMTiles, it can go far – theoretically the driver’s memory, your browser’s memory and your network is the limit. On Databricks, however, there is a quite frugal limit for the sizes of widgets, so we’ll have to limit the sample dataset a bit more, or you can generate a separate HTML file instead.\nNote that we use quite conservative limits, only a few hundred polygons – for vizualising more, see the HTML export option below.\n# Example dataset: Municipalities in the Netherlands via Overture Maps\ndf = (\n    spark.table(\"carto_overture_maps_divisions.carto.division_area\")\n    .selectExpr(\"geometry\", \"names:primary::string as name\")\n    .where(\"\"\"country = 'NL' and subtype='county'\"\"\")\n)\n\nspark_viz(df, wkb_col=\"geometry\", other_cols=[\"name\"], limit=1000)\nIf all went well and you see data on the map (not only the screenshot): note that you can not just zoom and pan, but can also click on a polygon to see the value of the non-geometry column(s).",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualize a geospatial Delta Lake table with Lonboard</span>"
    ]
  },
  {
    "objectID": "viz/lonboard.html#in-notebook-widget-example",
    "href": "viz/lonboard.html#in-notebook-widget-example",
    "title": "Visualize a geospatial Delta Lake table with Lonboard",
    "section": "",
    "text": "Tip\n\n\n\nThis example uses the CARTO/Overture Maps datasets that you can add to your workspace via the Marketplace.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe CARTO/Overture Maps tables are stored in us-west-2 as of writing, so if you are not using Databricks Free Edition and you are in any other region, you will have to pay egress charges based on the amount of data you read.\n\n\n\n\n\n\nlonboard\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you receive the error:\nCommand result size exceeds limit: Exceeded XXXX bytes (current = XXXX)\nThen you need to reduce your limit or tighten your filter.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualize a geospatial Delta Lake table with Lonboard</span>"
    ]
  },
  {
    "objectID": "viz/lonboard.html#export-to-html",
    "href": "viz/lonboard.html#export-to-html",
    "title": "Visualize a geospatial Delta Lake table with Lonboard",
    "section": "Export to HTML",
    "text": "Export to HTML\nTo see much more data at the same time (than what fits within Databricks’ widget size limits), we can generate a HTML file to a Volume, download and open that file. If you want to avoid downloading, you could serve it via Databricks Apps instead, see the html-viewer app here – note, however, the the data to visualize still needs to travel through the network to your browser anyway.\nYou will see that you can use this method to generate HTML files of hundreds of megabytes – so then the limit becomes your local memory for what your browser can open.\n\ndf = (\n    spark.table(\"carto_overture_maps_divisions.carto.division_area\")\n    .selectExpr(\"geometry\", \"names:primary::string as name\")\n    .where(\n        \"\"\"\n           country in ('BE', 'NL', 'LU', 'DE', 'FR', 'PL', 'CZ', 'SK', 'CH')\n           and subtype='county'\"\"\"\n    )\n    .orderBy(\"name\")  # just to make the results reproducible\n)\n\n\nspark_viz(\n    df,\n    wkb_col=\"geometry\",\n    other_cols=[\"name\"],\n    limit=100_000,\n    output_html=\"/Volumes/workspace/default/default/output.html\",\n)\n\nDownloading the above html and opening it, we can see much more data than what fits into an inline widget:\n\n\n\nlonboard html",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualize a geospatial Delta Lake table with Lonboard</span>"
    ]
  },
  {
    "objectID": "viz/lonboard.html#using-duckdb",
    "href": "viz/lonboard.html#using-duckdb",
    "title": "Visualize a geospatial Delta Lake table with Lonboard",
    "section": "Using DuckDB",
    "text": "Using DuckDB\nIf you are working with DuckDB (instead of Databricks SQL), you can directly feed in a DuckDB query to lonboard, without having to handle arrow yourself – after all, there’s already DuckDB used inside the above example.\nSo replicating the above example with DuckDB only, using the Overture Maps dataset on S3 (instead of the mirror on Databricks):\n\nviz(\n    duckdb.sql(\"\"\"\n        select geometry, names.primary as name\n        from read_parquet('s3://overturemaps-us-west-2/release/2025-07-23.0/theme=divisions/type=division_area/*')\n        where country = 'NL' and subtype='county'\"\"\")\n).as_html()\n\n\n\n\nlonboard",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualize a geospatial Delta Lake table with Lonboard</span>"
    ]
  },
  {
    "objectID": "viz/PMTiles.html",
    "href": "viz/PMTiles.html",
    "title": "Visualize with PMTiles",
    "section": "",
    "text": "PMTiles (a tileserver in the form of a single file, read with Range Requests) is a powerful way to visualize very large datasets, see https://pmtiles.io . The key tool to generate one is tippecanoe.\nSee the end-to-end example for a PMTiles generation and visualization example.\n\n\n\n\n\n\nNote\n\n\n\nNote that the maps built on PMTiles are slippy maps, pannable and zoomable, unlike the screenshot of it below.\n\n\n\n\n\nrailnetwork",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Visualize with PMTiles</span>"
    ]
  },
  {
    "objectID": "viz/QGIS.html",
    "href": "viz/QGIS.html",
    "title": "Visualizing Delta Lake geospatial data in QGIS",
    "section": "",
    "text": "Few pointers: - For moderate data sizes, the simplest way is to download your data in a format of your liking such as GeoParquet, GeoPackage, FlatGeobuf or similar. - For large datasets, see in the FlatGeobuf section how you can stream a FlatGeobuf file from Databricks volumes into QGIS. - last but not least, check out this plugin to connect QGIS directly to a SQL warehouse and read Delta Lake tables with GEOMETRY columns.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualizing Delta Lake geospatial data in QGIS</span>"
    ]
  },
  {
    "objectID": "stfunctions/native.html",
    "href": "stfunctions/native.html",
    "title": "Databricks Native ST geospatial functions",
    "section": "",
    "text": "Setup\nDatabricks SQL includes a large number of ST geospatial functions for large-scale, native processing of geodata. Just a few examples:\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\n\nspark.sql(f\"use {CATALOG}.{SCHEMA}\")",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Databricks Native ST geospatial functions</span>"
    ]
  },
  {
    "objectID": "stfunctions/native.html#geography-operations",
    "href": "stfunctions/native.html#geography-operations",
    "title": "Databricks Native ST geospatial functions",
    "section": "GEOGRAPHY operations",
    "text": "GEOGRAPHY operations\nYou can use st_area, st_length and st_perimeter directly on GEOGRAPHY (lon/lat) columns to get results in meters:\n\n\n\n\n\n\nTip\n\n\n\nThis example uses the CARTO/Overture Maps datasets that you can add to your workspace via the Marketplace.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe CARTO/Overture Maps tables are stored in us-west-2 as of writing, so if you are not using Databricks Free Edition and you are in any other region, you will have to pay egress charges based on the amount of data you read.\n\n\n\n%sql\n-- Areas of countries. Note that a country might not be listed if they cross the date line, leading to either missing geometry in the CARTO dataset already, or unsupported coordinates for `st_geogfromwkb` (hence the use of `try_to_geography`)\nselect\n  country,\n  names:primary name,\n  st_area(try_to_geography(geometry)) / 1e6 area_km2\nfrom\n  carto_overture_maps_divisions.carto.division_area\nwhere\n  subtype = 'country'\n  and class = 'land'\norder by\n  area_km2 desc\n-- Returns:\n-- country  name    area_km2\n-- CA   \"\"\"Canada\"\"\"    9966895.526025355\n-- US   \"\"\"United States\"\"\" 9476994.623136332\n-- CN   \"\"\"中国\"\"\"    9390439.241133066\n-- BR   \"\"\"Brasil\"\"\"    8507809.984099092\n-- IN   \"\"\"India\"\"\" 3149764.8857977577\n-- ...\n\nIf you store (lon, lat) point data as GEOGRAPHY, you can calculate distance in meters by converting to GEOMETRY(4326) first:\n\n%sql\nwith cities as (\n  select\n    names:primary::string as name,\n    st_geogfromwkb(geometry) geog\n  from\n    carto_overture_maps_divisions.carto.division\n  where\n    subtype = 'locality'\n    and class = 'city'\n    -- just to speed up the lookup\n    and `__carto_xmax` between - 1.41 and 2.99\n    and `__carto_ymax` between 48.57 and 52.21\n),\nlondon as (\n  select\n    *\n  from\n    cities\n  where\n    name = \"London\"\n),\nparis as (\n  select\n    *\n  from\n    cities\n  where\n    name = \"Paris\"\n)\nselect\n  st_distancespheroid(london.geog::geometry(4326), paris.geog::geometry(4326)) / 1e3 dist_km\nfrom\n  london,\n  paris\n--\n-- Returns:\n--\n-- dist_km\n-- 344.08532856606695",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Databricks Native ST geospatial functions</span>"
    ]
  },
  {
    "objectID": "stfunctions/native.html#geometry-operations",
    "href": "stfunctions/native.html#geometry-operations",
    "title": "Databricks Native ST geospatial functions",
    "section": "GEOMETRY operations",
    "text": "GEOMETRY operations\nWe can also calculate the distance in meters between two GEOMETRY lon-lat polygons (i.e. not just between points) if we transform to an appropriate local Cartesian coordinate system first:\n\n%sql\nwith dutch_provinces as (\n  select\n    names:primary::string as name,\n    -- 28992 is the SRID of the Dutch local Cartesian coordinate system:\n    -- https://epsg.io/28992\n    st_transform(st_geomfromwkb(geometry, 4326), 28992) geom_rd\n  from\n    carto_overture_maps_divisions.carto.division\n  where\n    subtype = 'region'\n    and country = 'NL'\n),\nnoord_holland as (\n  select\n    *\n  from\n    dutch_provinces\n  where\n    name = \"Noord-Holland\"\n),\nnoord_brabant as (\n  select\n    *\n  from\n    dutch_provinces\n  where\n    name = \"Noord-Brabant\"\n)\nselect\n  st_distance(noord_holland.geom_rd, noord_brabant.geom_rd) / 1e3 dist_km\nfrom\n  noord_holland,\n  noord_brabant\n--\n-- Returns:\n--\n-- dist_km\n-- 131.66041515751186\n\n\nIf you want to find the actual shortest line as well, not just the distance, see how to define UDFs with DuckDB.",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Databricks Native ST geospatial functions</span>"
    ]
  },
  {
    "objectID": "stfunctions/native.html#spatial-join",
    "href": "stfunctions/native.html#spatial-join",
    "title": "Databricks Native ST geospatial functions",
    "section": "Spatial join",
    "text": "Spatial join\nOptimizing for efficient spatial joins is out of scope here – you’d need to use some spatial indexing such as H3. (Databricks might add support to spatial joins of GEOMETRY/GEOGRAPHY types over time, but as of Aug 2025, using an explicit spatial index is highly benefitial.)\nWe were using the Delta Lake tables of Overture Maps provided by CARTO as our examples. Notice that these are clustered by the __carto_[x|y][min|max] columns:\n\n%sql\ndescribe extended carto_overture_maps_divisions.carto.division\n--\n-- Returns:\n--\n-- ...\n-- # Clustering Information\n-- # col_name   data_type\n-- __carto_xmin double\n-- __carto_xmax double\n-- __carto_ymin double\n-- __carto_ymax double\n\nSo in the case of these tables, we can make use of these columns for efficient joins, filtering for bounding boxes before calling st_contains (or st_intersects) for a precise join.\nLet’s fetch all buildings of Amsterdam:\n\n%sql\n-- capture the city bbox in a table\ncreate or replace table tmp_ams as\nselect\n  st_geomfromwkb(geometry) geometry,\n  `__carto_xmin`,\n  `__carto_xmax`,\n  `__carto_ymin`,\n  `__carto_ymax`\nfrom\n  carto_overture_maps_divisions.carto.division_area\nwhere\n  country = 'NL'\n  and subtype = 'county'\n  and names:primary::string = \"Amsterdam\"\n\nWe’ll fetch the bbox values from above to inject into the SQL query below – yes, it would be way more elegant to avoid hardcoding and use a join, but as of current testing, this is way faster.\n\nbbox = spark.table(\"tmp_ams\").toPandas().iloc[0,]\n\nspark.sql(f\"\"\"create or replace table tmp_ams_buildings_bbox as\nselect\n  building.* except (geometry),\n  st_geomfromwkb(building.geometry) building_geometry\nfrom\n  -- first approximate join on bounding boxes\n  carto_overture_maps_buildings.carto.building\nwhere\n  building.__carto_xmin between {bbox[\"__carto_xmin\"]} and {bbox[\"__carto_xmax\"]}\n  and building.__carto_ymin between {bbox[\"__carto_ymin\"]}\n    and {bbox[\"__carto_ymax\"]}\"\"\")\n\nNow we can filter with st_contains to leave out the false positives:\n\n%sql\ncreate or replace table tmp_ams_buildings as\nselect\n  b.*\nfrom\n  tmp_ams\n    join tmp_ams_buildings_bbox b\n      on -- then more precise filter on `st_contains`\n      st_contains(tmp_ams.geometry, building_geometry)\n\n\n%pip install duckdb lonboard shapely --quiet\n\n\ndef spark_viz(df, wkb_col=\"geometry\", other_cols=None, limit=10_000, output_html=None):\n    # needs `%pip install duckdb lonboard shapely`\n\n    if other_cols is None:\n        other_cols = []\n\n    import duckdb\n    from lonboard import viz\n\n    try:\n        duckdb.load_extension(\"spatial\")\n    except duckdb.duckdb.IOException:\n        duckdb.install_extension(\"spatial\")\n        duckdb.load_extension(\"spatial\")\n\n    dfa = df.select([wkb_col] + other_cols).limit(limit).toArrow()\n    if dfa.num_rows == limit:\n        print(f\"Data truncated to limit {limit}\")\n\n    query = duckdb.sql(\n        f\"\"\"select * replace (st_geomfromwkb({wkb_col}) as {wkb_col})\n        from dfa\n        where {wkb_col} is not null\"\"\"\n    )\n    if output_html is None:\n        return viz(query).as_html()\n    else:\n        viz(query).to_html(output_html)\n        return output_html\n\n\nspark_viz(\n    spark.table(\"tmp_ams_buildings\").selectExpr(\n        \"st_asbinary(building_geometry) as geometry\"\n    )\n)\n\n\n\n\nams_buildings_10k\n\n\nIf you want to see a much larger sample (or possibly the whole dataset), you’ll need to output to a HTML into Volumes, and either download the file and open it locally, or use a Databricks App such as this html-viewwer example:\n\nspark_viz(\n    spark.table(\"tmp_ams_buildings\").selectExpr(\n        \"st_asbinary(building_geometry) as geometry\"\n    ),\n    limit=1_000_000,\n    output_html=f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/ams_buildings.html\",\n)\n\n\n\n\nams-all-buildings",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Databricks Native ST geospatial functions</span>"
    ]
  },
  {
    "objectID": "stfunctions/native.html#cleanup",
    "href": "stfunctions/native.html#cleanup",
    "title": "Databricks Native ST geospatial functions",
    "section": "Cleanup",
    "text": "Cleanup\n\n%sql\n -- drop table tmp_ams\n -- drop table tmp_ams_buildings_bbox\n -- drop table tmp_ams_buildings",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Databricks Native ST geospatial functions</span>"
    ]
  },
  {
    "objectID": "stfunctions/duckdb_udf.html",
    "href": "stfunctions/duckdb_udf.html",
    "title": "Define a Spark UDF using a DuckDB Spatial function",
    "section": "",
    "text": "Setup\n%md\nDatabricks SQL now contains lots of ST Functions. However, at some point you might just need a geospatial function not (yet) available natively in Databricks, but maybe available in DuckDB Spatial. For example, as of Aug 2025, the st_shortestline function.\nThen we can register the DuckDB function as a Spark UDF as follows:\n%pip install duckdb lonboard shapely --quiet\n\nimport duckdb\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import BinaryType",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Define a Spark UDF using a DuckDB Spatial function</span>"
    ]
  },
  {
    "objectID": "stfunctions/duckdb_udf.html#define-the-spark-udf-based-on-a-duckdb-spatial-function",
    "href": "stfunctions/duckdb_udf.html#define-the-spark-udf-based-on-a-duckdb-spatial-function",
    "title": "Define a Spark UDF using a DuckDB Spatial function",
    "section": "Define the Spark UDF based on a DuckDB Spatial function",
    "text": "Define the Spark UDF based on a DuckDB Spatial function\n\ndef shortestline_func(a: pd.Series, b: pd.Series) -&gt; pd.Series:\n    try:\n        import duckdb\n        from duckdb import ColumnExpression, FunctionExpression\n    except ModuleNotFoundError as mfe:\n        raise Exception(\n            \"'duckdb' not installed: run '%pip install duckdb' first.\"\n        ) from mfe\n\n    duckdb.sql(\"SET extension_directory = '/tmp/duckdb_ext'\")\n    try:\n        duckdb.load_extension(\"spatial\")\n    except duckdb.duckdb.IOException:\n        duckdb.install_extension(\"spatial\")\n        duckdb.load_extension(\"spatial\")\n\n    df = pd.DataFrame({\"a\": a, \"b\": b})  # noqa: F841\n\n    res = (\n        duckdb.df(df)\n        .select(\n            FunctionExpression(\n                \"st_aswkb\",\n                FunctionExpression(\n                    \"st_shortestline\",\n                    FunctionExpression(\"st_geomfromwkb\", ColumnExpression(\"a\")),\n                    FunctionExpression(\"st_geomfromwkb\", ColumnExpression(\"b\")),\n                ),\n            ).alias(\"res\")\n        )\n        .df()[\"res\"]\n    )\n\n    return res\n\n\nst_duckdb_shortestline = F.pandas_udf(shortestline_func, returnType=BinaryType())\n\n\nspark.udf.register(\"st_duckdb_shortestline\", st_duckdb_shortestline)",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Define a Spark UDF using a DuckDB Spatial function</span>"
    ]
  },
  {
    "objectID": "stfunctions/duckdb_udf.html#usage-example",
    "href": "stfunctions/duckdb_udf.html#usage-example",
    "title": "Define a Spark UDF using a DuckDB Spatial function",
    "section": "Usage example",
    "text": "Usage example\n\n\n\n\n\n\nTip\n\n\n\nThis example uses the CARTO/Overture Maps datasets that you can add to your workspace via the Marketplace.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe CARTO/Overture Maps tables are stored in us-west-2 as of writing, so if you are not using Databricks Free Edition and you are in any other region, you will have to pay egress charges based on the amount of data you read.\n\n\n\n%sql\ncreate or replace temporary view lamanche as\nwith england as (\n  select\n    geometry\n  from\n    carto_overture_maps_divisions.carto.division_area\n  where\n    subtype = 'country'\n    and country = 'GB'\n    and class = 'land'\n),\nfrance as (\n  select\n    geometry\n  from\n    carto_overture_maps_divisions.carto.division_area\n  where\n    subtype = 'country'\n    and country = 'FR'\n    and class = 'land'\n)\nselect\n  st_duckdb_shortestline(england.geometry, france.geometry) geometry\nfrom\n  england,\n  france\n\n\n\n\n\n\n\nNote\n\n\n\nNit: To correctly calculate shortestline in the Eucledian sense, we should actually transform lon/lat to an SRID that maintains the angles, such as EPSG:3857. (Then, if we were interested in the length of it in meters, we could transform back to lon/lat and use st_distancespheroid.)\nTo keep this example simple, we’ll now calculate the shortest line in the lat/lon coordinate system – we will get a quite similar result in this example of Calais-Dover.",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Define a Spark UDF using a DuckDB Spatial function</span>"
    ]
  },
  {
    "objectID": "stfunctions/duckdb_udf.html#visualizing-the-result",
    "href": "stfunctions/duckdb_udf.html#visualizing-the-result",
    "title": "Define a Spark UDF using a DuckDB Spatial function",
    "section": "Visualizing the result",
    "text": "Visualizing the result\nLet’s visualize the result with Lonboard:\n\ndef spark_viz(df, wkb_col=\"geometry\", other_cols=None, limit=10_000, output_html=None):\n    # needs `%pip install duckdb lonboard shapely`\n\n    if other_cols is None:\n        other_cols = []\n\n    from lonboard import viz\n\n    try:\n        duckdb.load_extension(\"spatial\")\n    except duckdb.duckdb.IOException:\n        duckdb.install_extension(\"spatial\")\n        duckdb.load_extension(\"spatial\")\n\n    dfa = df.select([wkb_col] + other_cols).limit(limit).toArrow()\n    if dfa.num_rows == limit:\n        print(f\"Data truncated to limit {limit}\")\n\n    query = duckdb.sql(\n        f\"\"\"select * replace (st_geomfromwkb({wkb_col}) as {wkb_col})\n        from dfa\n        where {wkb_col} is not null\"\"\"\n    )\n    if output_html is None:\n        return viz(query).as_html()\n    else:\n        viz(query).to_html(output_html)\n        return output_html\n\n\nspark_viz(spark.table(\"lamanche\"))\n\n\n\n\nlamanche",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Define a Spark UDF using a DuckDB Spatial function</span>"
    ]
  },
  {
    "objectID": "stfunctions/GeoPandas.html",
    "href": "stfunctions/GeoPandas.html",
    "title": "GeoPandas",
    "section": "",
    "text": "If you are processing geospatial data that’s small enough to fit in memory, then besides (Databricks SQL and) DuckDB Spatial, you can also consider using GeoPandas. For more scalable approaches, you could define Spark UDFs based on GeoPandas similar to how we defined them on top of DuckDB Spatial. As this guide is mostly focused on SQL-based approaches, we won’t go into more detail here.",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>GeoPandas</span>"
    ]
  },
  {
    "objectID": "end2end/train_to_slopes.html",
    "href": "end2end/train_to_slopes.html",
    "title": "End-to-end example: Taking the train to the slopes",
    "section": "",
    "text": "Fetching OSM\nGeospatial analysis often includes combining multiple data sources. In this example, we will only use OpenStreetMap (OSM), but as you’ll see one of the datasets is a LineString type and another is a Polygon type.\nLet’s say we’d like to go skiing but would like to avoid the hassle of driving or flying, Are there slopes accessible by train? It turns out, yes – let’s find them. We’ll use France as an example but you could just as well try to change it to another (Alpine) country.\nSee also Importing other formats on how to use DuckDB Spatial to read OSM and other file types.\n%pip install \"databricks-connect&gt;=17.1.0\" duckdb lonboard shapely --quiet\n%restart_python\nimport duckdb\n\nduckdb.sql(\"install spatial; load spatial;\")\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\n\nCONTINENT = \"europe\"\nCOUNTRY = \"france\"\nGEOFABRIK_URL = f\"https://download.geofabrik.de/{CONTINENT}/{COUNTRY}-latest.osm.pbf\"\n\nspark.sql(f\"use {CATALOG}.{SCHEMA}\")\nspark.sql(f\"create volume if not exists {CATALOG}.{SCHEMA}.{VOLUME}\")\nfile_name = GEOFABRIK_URL.split(\"/\")[-1]\nfile_basename = file_name.rsplit(\".\")[0]\nvolume_file_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{file_name}\"\nvolume_parquet_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{file_basename}.parquet\"\ntablename = f\"planet_osm_{COUNTRY}\"\n# If this is too slow, you can \"cheat\" and download only\n# https://download.geofabrik.de/europe/france/rhone-alpes.html instead -- after all,\n# most French ski resorts are in that region.\n!curl -o {volume_file_path} {GEOFABRIK_URL}\nduckdb.sql(\n    f\"\"\"\ncopy (\n    select\n        *\n    from\n        '{volume_file_path}'\n) to '{volume_parquet_path}'\n(format parquet)\n;\n\"\"\"\n)\nspark.read.parquet(volume_parquet_path).write.option(\"clusteredBy\", \"id\").mode(\n    \"overwrite\"\n).saveAsTable(f\"{CATALOG}.{SCHEMA}.{tablename}\")",
    "crumbs": [
      "End-to-end examples",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>End-to-end example: Taking the train to the slopes</span>"
    ]
  },
  {
    "objectID": "end2end/train_to_slopes.html#transform-into-a-table-with-geometry-with-databricks-spatial-sql",
    "href": "end2end/train_to_slopes.html#transform-into-a-table-with-geometry-with-databricks-spatial-sql",
    "title": "End-to-end example: Taking the train to the slopes",
    "section": "Transform into a table with GEOMETRY with Databricks Spatial SQL",
    "text": "Transform into a table with GEOMETRY with Databricks Spatial SQL\nSee also Databricks Spatial SQL ST functions, and Delta Lake tables with GEOMETRY.\n\n%sql\ncreate or replace table skiresorts as\nwith wintersports as (\n  select\n    id as wintersports_id,\n    tags.name,\n    posexplode(refs) as (pos, id)\n  from\n    planet_osm_france\n  where\n    kind = \"way\"\n    and tags.landuse = \"winter_sports\"\n)\nselect\n  wintersports_id,\n  name,\n  st_makepolygon(\n    st_makeline(\n      transform(sort_array(array_agg(struct(pos, lon, lat))), x -&gt; st_point(x.lon, x.lat, 4326))\n    )\n  ) geometry\nfrom\n  wintersports join planet_osm_france p using (id)\ngroup by\n  all\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure you are using Serverless environment version 4+, or else outputting GEOMETRY/GEOGRAPHY types directly (without e.g. st_asewkt()) will not work.\n\n\n\n%sql\nfrom\n  skiresorts\n-- Returns:\n-- wintersports_id  name    geometry\n-- 23079840 Isola 2000  SRID=4326;POLYGON((7.1524774 44.204359200000006,...))\n-- 27163365 Estación de Esquí Aramón Formigal   SRID=4326;POLYGON((-0.41821430000000004 42.8009106,...))\n-- ...",
    "crumbs": [
      "End-to-end examples",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>End-to-end example: Taking the train to the slopes</span>"
    ]
  },
  {
    "objectID": "end2end/train_to_slopes.html#visualize-with-lonboard",
    "href": "end2end/train_to_slopes.html#visualize-with-lonboard",
    "title": "End-to-end example: Taking the train to the slopes",
    "section": "Visualize with Lonboard",
    "text": "Visualize with Lonboard\nSee also Visualize with Lonboard.\n\nfrom lonboard import viz\nfrom pyspark.sql import functions as F\n\ndfa = (\n    spark.table(\"skiresorts\")\n    .withColumn(\"geometry\", F.expr(\"st_asbinary(geometry)\"))\n    .toArrow()\n)\nviz(duckdb.sql(\"select name, st_geomfromwkb(geometry) geometry from dfa\")).as_html()\n\n\n\n\nwintersport",
    "crumbs": [
      "End-to-end examples",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>End-to-end example: Taking the train to the slopes</span>"
    ]
  },
  {
    "objectID": "end2end/train_to_slopes.html#load-the-train-network",
    "href": "end2end/train_to_slopes.html#load-the-train-network",
    "title": "End-to-end example: Taking the train to the slopes",
    "section": "Load the train network",
    "text": "Load the train network\nWe will use similar techniques as above to load the railway network as well.\n\n%sql\nwith t1 as (\n  select\n    tags.name,\n    id as trainroute_id,\n    posexplode(refs) as (pos, id)\n  from\n    planet_osm_france\n  where\n    tags.type = 'route'\n    and tags.route = 'train'\n    and id = 2274158\n),\nt2 as (\n  select\n    t1.name,\n    p.id route_id,\n    p.* --,\n  -- posexplode(arrays_zip(p.refs, p.ref_roles, p.ref_types)) as (pos, ref),\n  -- ref[\"refs\"] id,\n  -- ref[\"ref_roles\"] role,\n  -- ref[\"ref_types\"] type\n  from\n    t1 join planet_osm_france p using (id)\n  where\n    p.tags.railway = 'rail'\n)\nselect\n  *\nfrom\n  t2\n\n\n%sql\ncreate or replace table train_routes as\nwith t1 as (\n  select\n    tags.name,\n    id as trainroute_id,\n    posexplode(refs) as (pos, id)\n  from\n    planet_osm_france\n  where\n    tags.type = 'route'\n    and tags.route = 'train'\n),\nt2 as (\n  select\n    t1.name,\n    p.id route_id,\n    posexplode(refs) as (pos, id)\n  from\n    t1 join planet_osm_france p using (id)\n  where\n    p.tags.railway = 'rail'\n)\nselect\n  t2.name,\n  route_id,\n  st_makeline(\n    transform(sort_array(array_agg(struct(pos, lon, lat))), x -&gt; st_point(x.lon, x.lat, 4326))\n  ) geometry\nfrom\n  t2 join planet_osm_france p using (id)\ngroup by\n  all\n\n\nspark.table(\"train_routes\").withColumn(\n    \"geometry\", F.expr(\"st_asewkt(geometry)\")\n).display()\n# Returns:\n\n# [lots of LINESTRING's]\n\nLet’s try to visualize the same way as we did for the ski domaines (spoiler alert: it might not work):\n\ndfa = (\n    spark.table(\"train_routes\")\n    .withColumn(\"geometry\", F.expr(\"st_asbinary(geometry)\"))\n    .toArrow()\n)\nviz(duckdb.sql(\"select name, st_geomfromwkb(geometry) geometry from dfa\")).as_html()\n# Returns:\n# Command result size exceeds limit: Exceeded 20971520 bytes (current = 20971797)",
    "crumbs": [
      "End-to-end examples",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>End-to-end example: Taking the train to the slopes</span>"
    ]
  },
  {
    "objectID": "end2end/train_to_slopes.html#visualize-with-pmtiles",
    "href": "end2end/train_to_slopes.html#visualize-with-pmtiles",
    "title": "End-to-end example: Taking the train to the slopes",
    "section": "Visualize with PMTiles",
    "text": "Visualize with PMTiles\nThe above visualization probably failed, due to the dataset being too large for the widget in Databricks. While for medium sized datasets there’s another workaround by saving the Lonboard output to a seperate HTML file via to_html() (detailed here), let’s instead use PMTiles instead that can work also for very large datasets.\nSee also the tippecanoe example here, which we will follow now via an intermediate Parquet and FlatGeobuf file.\nWe will actually visualize both the train routes and the ski resorts at the same time below.\n\n%sql\ncreate temporary view skiresorts_with_trainroutes as\nselect\n  null as route_id,\n  wintersports_id,\n  name,\n  geometry\nfrom\n  skiresorts\nunion all\nselect\n  route_id,\n  null as wintersports_id,\n  name,\n  geometry\nfrom\n  train_routes\n\n\n# Write to parquet\nspark.table(\"skiresorts_with_trainroutes\").write.mode(\"overwrite\").parquet(\n    f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet\"\n)\n\n\n# Write FlatGeobuf\nquery = f\"\"\"\ncopy (\nselect \n    * replace (st_geomfromwkb(geometry) as geometry)\nfrom\n    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n) to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb'\n(FORMAT GDAL, DRIVER flatgeobuf, LAYER_CREATION_OPTIONS 'TEMPORARY_DIR=/tmp/')\"\"\"\nduckdb.sql(query)\n\n\n%sh\n# Install tippecanoe\n\n# ~3.5 min on high memory serverless https://docs.databricks.com/aws/en/compute/serverless/dependencies#high-memory\ngit clone https://github.com/felt/tippecanoe.git\ncd tippecanoe\nmake -j\nmake install PREFIX=$HOME/.local\nrm -r tippecanoe\n\n\nimport os\n\nHOME = os.environ[\"HOME\"]\n\n# see https://github.com/felt/tippecanoe/blob/main/README.md#try-this-first and e.g.\n# https://github.com/OvertureMaps/overture-tiles/blob/main/scripts/2024-07-22/places.sh\n# for possible options\n!{HOME}/.local/bin/tippecanoe -zg -rg -o /tmp/geometries.pmtiles  --simplification=10 --drop-smallest-as-needed --drop-densest-as-needed --extend-zooms-if-still-dropping --maximum-tile-bytes=2500000 --progress-interval=10 -l geometries --force /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb\n# NOTE: this mv will emit an error related to updating metadata (\"mv: preserving\n# permissions for ‘[...]’: Operation not permitted\"), this can be ignored.\n!mv /tmp/geometries.pmtiles /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.pmtiles\n\nAnd, that’s it! You can visualize the pmtiles file by downloading and uploading to https://pmtiles.io , or much better, by using a PMTiles viewer via Databricks Apps, an example implementation is here and your result would look like this:\n\n\n\n\n\n\nNote\n\n\n\nNote that the maps built on PMTiles are slippy maps, pannable and zoomable, unlike the screenshot of it below.\n\n\n\n\n\nrailnetwork",
    "crumbs": [
      "End-to-end examples",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>End-to-end example: Taking the train to the slopes</span>"
    ]
  },
  {
    "objectID": "end2end/train_to_slopes.html#spatial-join",
    "href": "end2end/train_to_slopes.html#spatial-join",
    "title": "End-to-end example: Taking the train to the slopes",
    "section": "Spatial join",
    "text": "Spatial join\nTo actually answer our question of which ski resorts are closest to a train line, we will also need to use a locally valid coordinate system, for st_buffer and similar functions to be able to work in meters, not degrees. In case of France being an example, probably EPSG:2154 is a suitable one.\nNow let’s say we want to find train routes at a maximum distance (say, 1km) of any ski resort. (Ideally we’d want to find train stations, but that’s too easy and in order to illustrate working with linestrings, let’s stay with routes. Think of it as a lower bound on distance.) We can thus buffer the ski resort polygons with the maximum distance, and intersect them with the routes, using H3 spatial indexing to expedite the join:\n\n%sql\ncreate\nor replace table skiresorts_buffered_h3 cluster by (cellid) as with buffered as (\n  select\n    *,\n    st_transform(\n      st_buffer(st_transform(geometry, 2154), 1000),\n      4326\n    ) :: GEOGRAPHY(4326) as geography_buffered\n  from\n    skiresorts\n),\ntessellated as (\n  select\n    *,\n    explode(h3_tessellateaswkb(geography_buffered, 8)) h3_8\n  from\n    buffered\n)\nselect\n  *\nexcept\n  (h3_8),\n  h3_8.cellid,\n  h3_8.core,\n  st_geomfromwkb(h3_8.chip) chip\nfrom\n  tessellated\n\n\n%sql\ncreate\nor replace table trainroutes_h3 cluster by (cellid) as with tessellated as (\n  select\n    *,\n    explode(\n      h3_tessellateaswkb(geometry :: GEOGRAPHY(4326), 8)\n    ) h3_8\n  from\n    train_routes\n)\nselect\n  *\nexcept\n  (h3_8),\n  h3_8.cellid,\n  h3_8.core,\n  st_geomfromwkb(h3_8.chip) chip\nfrom\n  tessellated\n\n\n%sql\ncreate or replace table closest_resorts as\nselect\n  wintersports_id,\n  s.name,\n  array_agg(route_id) route_ids\nfrom\n  skiresorts_buffered_h3 s join trainroutes_h3 t using (cellid)\nwhere\n  st_intersects(s.chip, t.chip)\ngroup by\n  all;\n\nselect\n  *\nfrom\n  closest_resorts\n-- Returns:\n-- wintersports_id  name    route_ids\n-- 599436073    Serre Chevalier [444359203,...]\n-- 589005175    Les Arcs / Peisey-Vallandry [171202612,...]\n-- 758151764    Le Lioran   [88691173,...]\n\nAnd that’s it, we found the 11 ski resorts closest to (i.e. within 1 km) train services! I leave it as an exercise to the reader to calculate the actual distance between the (unbuffered) resort polygons and the train route linestrings – don’t forget to use the geometry transformed to the local coordinate system, otherwise st_distance will work with degrees (and st_distancespheroid only works within points as of writing).\nIf you were interested in, say, calculating the actual shortest path between such geometries, you could use a Spark UDF with DuckDB.\nLet’s visualize the winners once again:\n\n%sql\ncreate or replace temporary view closest_resorts_geo as\nselect\n  wintersports_id,\n  skiresorts.name as resort_name,\n  geometry\nfrom\n  skiresorts join closest_resorts using (wintersports_id)\nunion all\nselect\n  null as wintersports_id,\n  null as resort_name,\n  geometry\nfrom\n  train_routes\n    join (\n      select\n        explode(route_ids) route_id\n      from\n        closest_resorts\n    )\n      using (route_id)\n\n\ndfa = (\n    spark.table(\"closest_resorts_geo\")\n    .withColumn(\"geometry\", F.expr(\"st_asbinary(geometry)\"))\n    .toArrow()\n)\nviz(\n    duckdb.sql(\"select * replace(st_geomfromwkb(geometry) as geometry) from dfa\")\n).as_html()\n\n\n\n\nclosest-resorts",
    "crumbs": [
      "End-to-end examples",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>End-to-end example: Taking the train to the slopes</span>"
    ]
  },
  {
    "objectID": "mapmatching/databricks_graphhopper.html",
    "href": "mapmatching/databricks_graphhopper.html",
    "title": "Run GraphHopper on Databricks",
    "section": "",
    "text": "Note that there is no particular advantage of running GraphHopper on Databricks, but if it is part of your larger workflow, then it is practical to keep this step in the same environment.\n\n# Replace the below parameters with the volume path you use.\n\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\nVOLDIR = \"graphhopper\"\n\nvolpath = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{VOLDIR}\"\n\nspark.sql(f\"create volume if not exists {CATALOG}.{SCHEMA}.{VOLUME}\")\n!mkdir -p /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{VOLDIR}\n\n\nimport os\n\nos.environ[\"VOLPATH\"] = volpath\n\n\n%sh\nrm -rf ${VOLPATH}\nmkdir ${VOLPATH}\ncd ${VOLPATH}\n\nwget -nv https://github.com/graphhopper/graphhopper/releases/download/8.0/graphhopper-web-8.0.jar\n\n# example config, trace, and matching pbf\nwget -nv https://github.com/graphhopper/graphhopper/raw/refs/tags/8.0/config-example.yml\nwget -nv https://raw.githubusercontent.com/graphhopper/graphhopper/refs/tags/8.0/web/src/test/resources/issue-13.gpx\nwget -nv https://download.geofabrik.de/europe/turkey-latest.osm.pbf\n\n#edit config to match the pbf\nsed -i \"s|datareader\\.file: \\\"\\\"|datareader.file: \\\"turkey-latest.osm.pbf\\\"|\" config-example.yml\n\n# note that this fails on many of the other gpx examples within the same folder, but that's a different problem.\njava -jar graphhopper-web-8.0.jar match --file config-example.yml --profile car issue-13.gpx\n\nVisualizing the generated GPX file with GPX Studio:\n\n\n\nissue-13 matched",
    "crumbs": [
      "Mapmatching",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Run GraphHopper on Databricks</span>"
    ]
  },
  {
    "objectID": "apps/databricks_apps.html",
    "href": "apps/databricks_apps.html",
    "title": "Helper apps on Databricks Apps",
    "section": "",
    "text": "pmtiles-viewer\nTo install one of these apps: follow the steps in the docs and sync one of the below directories to your app diredtory.\nhttps://github.com/danielsparing/databricks-geospatial-cookbook/tree/main/apps/pmtiles-viewer\nThis app is a viewer of PMTiles files stored in Volumes.\nUses App authorization.\nFor example, if you set up your app at https://pmtiles-viewer-{workspace_id}.aws.databricksapps.com/ and your PMTiles file is stored at the root of a Volume at /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.pmtiles, then your PMTiles viewer URL would be:",
    "crumbs": [
      "Helper apps on Databricks Apps",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Helper apps on Databricks Apps</span>"
    ]
  },
  {
    "objectID": "apps/databricks_apps.html#pmtiles-viewer",
    "href": "apps/databricks_apps.html#pmtiles-viewer",
    "title": "Helper apps on Databricks Apps",
    "section": "",
    "text": "https://pmtiles-viewer-{workspace_id}.aws.databricksapps.com/?filePath=/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.pmtiles",
    "crumbs": [
      "Helper apps on Databricks Apps",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Helper apps on Databricks Apps</span>"
    ]
  },
  {
    "objectID": "apps/databricks_apps.html#html-viewer",
    "href": "apps/databricks_apps.html#html-viewer",
    "title": "Helper apps on Databricks Apps",
    "section": "html-viewer",
    "text": "html-viewer\nhttps://github.com/danielsparing/databricks-geospatial-cookbook/tree/main/apps/html-viewer\nThis app uses user authorization (in Preview as of Aug 2025, and not yet available on Free Edition) to open a HTML file stored in Volumes.\nAn example use case is Lonboard generating a geospatial visualization into a HTML file, and then this app can be used to view this file without download – not that this is not that useful after all, as the data still has to travel to the user’s browser.",
    "crumbs": [
      "Helper apps on Databricks Apps",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Helper apps on Databricks Apps</span>"
    ]
  },
  {
    "objectID": "appendix/databricks_ogr2ogr_parquet.html",
    "href": "appendix/databricks_ogr2ogr_parquet.html",
    "title": "How to install ogr2ogr in a Databricks notebook (incl. Parquet support)",
    "section": "",
    "text": "ogr2ogr is a geospatial file conversion tool, part of GDAL. For example, you can use it to read in a directory of GML (geo XML) files, and write them out to GeoPackage (.gpkg), or even GeoParquet. You could first try the duckdb way (also using GDAL under the hood), but in some more complex cases you might need to use gdal CLI yourself.\nIn theory, on a classic (non-serverless) Compute, we could just run apt-get install -y gdal-bin, but as of Aug 2025 this would not include Parquet support yet. Instead, the libgdal-arrow-parquet extension package that we need can be installed via conda-forge. So let’s first install conda-forge 1:\nNow we can add arrow/parquet support:\nAnd that’s it:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>How to install `ogr2ogr` in a Databricks notebook (incl. Parquet support)</span>"
    ]
  },
  {
    "objectID": "appendix/databricks_ogr2ogr_parquet.html#footnotes",
    "href": "appendix/databricks_ogr2ogr_parquet.html#footnotes",
    "title": "How to install ogr2ogr in a Databricks notebook (incl. Parquet support)",
    "section": "",
    "text": "The curl download link comes from conda-forge and their installation instructions on GitHub.↩︎",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>How to install `ogr2ogr` in a Databricks notebook (incl. Parquet support)</span>"
    ]
  },
  {
    "objectID": "appendix/https_install_duckdbextension.html",
    "href": "appendix/https_install_duckdbextension.html",
    "title": "Install DuckDB Extensions via HTTPS",
    "section": "",
    "text": "Note\n\n\n\nThis issue should not come up on Databricks Free Edition, as there you’re using Serverless and the network is set up by Databricks.\n\n\nIf your network blocks HTTP requests, you cannot simply install DuckDB extensions by e.g. INSTALL spatial; (it will fail after ~10 minutes), because of the dependency on httpfs, an extension itself – see details here.\nThe workaround is to separately download the httpfs extension:\n\n%pip install duckdb --quiet\n\nimport os\nimport platform\nfrom urllib.parse import urlparse\n\nimport duckdb\nimport requests\n\narch = platform.machine()\nif arch == \"x86_64\":\n    architecture = \"linux_amd64\"\nelif arch == \"aarch64\":\n    architecture = \"linux_arm64\"\nelse:\n    raise Exception(f\"unknown_arch: {arch}\")\n\nduckdb_version = duckdb.__version__\nurl = f\"https://extensions.duckdb.org/v{duckdb_version}/{architecture}/httpfs.duckdb_extension.gz\"\n\noutput_file = os.path.basename(urlparse(url).path)\nresponse = requests.get(url, timeout=30)\nresponse.raise_for_status()\nwith open(output_file, \"wb\") as f:\n    f.write(response.content)\n\nduckdb.install_extension(output_file)\n\nos.remove(output_file)\n\nduckdb.sql(\"SET custom_extension_repository='https://extensions.duckdb.org'\")\n\nAnd now you can install other extensions, such as:\n\nduckdb.sql(\"install spatial; load spatial\")",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Install DuckDB Extensions via HTTPS</span>"
    ]
  }
]