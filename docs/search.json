[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "1 Introduction\n\nGeospatial (vector) data cookbook using Databricks Spatial SQL with a helping hand from DuckDB Spatial Extension where needed.\nAll pages are available as IPython notebooks as well in the source GitHub repository and they were successfully tested on Databricks Free Edition, Serverless environment version 3 unless otherwise noted.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html",
    "title": "2  DuckDB on Databricks",
    "section": "",
    "text": "2.1 Setting up DuckDB on Databricks\nDuckDB is a formidable new single-machine analytics tool, tracing its origins to the same Dutch research institute as Python. Crucially for this guide, it comes with a remarkably good Spatial extension.\nWhile Databricks comes with its own set of geospatial features, such as ST functions and H3 functions, nothing stops you to use DuckDB on the side as well.\n(What you do have to keep in mind though is that while much of Databricks’s tooling, namely Apache Spark, is focused on big data analysis multi-node clusters, your DuckDB instead will just run on single-node, just like e.g. Pandas would. So use single-node clusters, or Spark UDFs [TODO: insert a link here before].)\n%pip install duckdb --quiet\n\nimport duckdb\n\n# Install the Spatial Extension:\nduckdb.sql(\"install spatial; load spatial\")\nThis allows you to directly use the DuckDB Spatial, for example:\nduckdb.sql(\"select st_distance(st_point(3, 0), st_point(0, 4)) d\")\n\n# Returns:\n\n# ┌────────┐\n# │   d    │\n# │ double │\n# ├────────┤\n# │    5.0 │\n# └────────┘",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#setting-up-duckdb-on-databricks",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#setting-up-duckdb-on-databricks",
    "title": "2  DuckDB on Databricks",
    "section": "",
    "text": "Note\n\n\n\nIf install spatial fails, check whether HTTP is blocked on your (corporate) network. If so, then you need to work around it as described here.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#visualize-duckdb-spatial-output",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#visualize-duckdb-spatial-output",
    "title": "2  DuckDB on Databricks",
    "section": "2.2 Visualize DuckDB Spatial output",
    "text": "2.2 Visualize DuckDB Spatial output\nIf your data is simply lon/lat points, you can make use of the built-in point map visualization in Databricks Notebooks if you convert the DuckDB to a Spark DataFrame via Pandas. Once the result is shown, click on the + icon right of the Table tab to add the visualization “Map (Markers)” such as the one shown on the below image.\nFollowing the New York City pizza restaurants example, but let’s switch to Amsterdam because why not:\n\nquery = duckdb.sql(\n    \"\"\"\nwith t as (\n  SELECT\n    id,\n    names.primary as name,\n    confidence AS confidence,\n    CAST(socials AS JSON) as socials,\n    geometry\n  FROM\n    read_parquet('s3://overturemaps-us-west-2/release/2025-07-23.0/theme=places/type=place/*')\n  WHERE\n    categories.primary = 'pizza_restaurant'\n    AND bbox.xmin BETWEEN 4.7 AND 5.0\n    AND bbox.ymin BETWEEN 52.3 AND 52.4\n)\nselect st_x(geometry) lon, st_y(geometry) lat, name from t\n\"\"\"\n)\nquery\n\n# Returns:\n\n# ┌───────────┬────────────┬──────────────────────────────────────────┐\n# │    lon    │    lat     │                   name                   │\n# │  double   │   double   │                 varchar                  │\n# ├───────────┼────────────┼──────────────────────────────────────────┤\n# │  4.762994 │ 52.3099144 │ Per Tutti                                │\n# │ 4.7789755 │ 52.3381557 │ New York Pizza                           │\n# │ 4.7811585 │ 52.3367951 │ CiCi Pizza                               │\n# │     ·     │      ·     │       ·                                  │\n# │     ·     │      ·     │       ·                                  │\n# │     ·     │      ·     │       ·                                  │\n\n\nspark.createDataFrame(query.df()).display()\n\n\n\n\npoint_map\n\n\nOr visualize with lonboard, which will work also for other geometry types like linestrings and polygons, again following an Overture Maps example:\n\n%pip install lonboard shapely --quiet\n\nfrom lonboard import viz\n\n\nquery = duckdb.sql(\n    \"\"\"\n  SELECT\n    subtype,\n    names.primary as name,\n    geometry\n  FROM\n    read_parquet('s3://overturemaps-us-west-2/release/2025-07-23.0/theme=divisions/type=division_area/*')\n  WHERE\n    4.7 &lt; bbox.xmax AND bbox.xmin &lt; 5.0\n    AND 52.3 &lt; bbox.ymax AND bbox.ymin &lt; 52.4\n    AND subtype = 'county'\n    AND country = 'NL'\n\"\"\"\n)\n\nviz(query).as_html()\n\n\n\n\nlonboard_gemeente_map\n\n\nNote that clicking on a polygon opens a table with its parameters, in this case the municipalities of Amsterdam and some of its neighbors.\nAs powerful as Lonboard is, it won’t be able to visualize extremely large numbers of geometries, so if you try and fail at a larger example, try filtering your objects further.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#write-delta-lake-tables-from-duckdb",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#write-delta-lake-tables-from-duckdb",
    "title": "2  DuckDB on Databricks",
    "section": "2.3 Write Delta Lake Tables from DuckDB",
    "text": "2.3 Write Delta Lake Tables from DuckDB\nIf you want to write a result to a delta lake table (or temporary view), you can use Pandas as an intermediary format:\n\nspark.createDataFrame(query.df()).createOrReplaceTempView(\"t\")\n\n# Or write a persistent table, instead of a temporary view:\n# spark.createDataFrame(query.df()).write.saveAsTable(\"t\")\n\n\n%sql\nselect name from t\n\n-- Returns:\n\n-- ┌────────────────┐\n-- │      name      │\n-- ├────────────────┤\n-- │ Haarlemmermeer │\n-- │ Aalsmeer       │\n-- │ De Ronde Venen │\n-- │ Amstelveen     │\n-- │ Ouder-Amstel   │\n-- │ Amsterdam      │\n-- │ Diemen         │\n-- │ Waterland      │\n-- └────────────────┘",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#read-delta-lake-tables-with-duckdb",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#read-delta-lake-tables-with-duckdb",
    "title": "2  DuckDB on Databricks",
    "section": "2.4 Read Delta Lake Tables with DuckDB",
    "text": "2.4 Read Delta Lake Tables with DuckDB\nWe can read moderate amount of data from a delta table to duckdb via Arrow (or Pandas). (This assumes that the data volume is not prohibitively large to load into the memory of a single machine.)\n\ndfa = spark.read.table(\"t\").toArrow()\n\nquery = duckdb.sql(\"\"\"\nselect\n    name\nfrom\n    dfa;\n\"\"\")\nquery\n\n# Returns:\n\n# ┌────────────────┐\n# │      name      │\n# │    varchar     │\n# ├────────────────┤\n# │ Haarlemmermeer │\n# │ Aalsmeer       │\n# │ De Ronde Venen │\n# │ Amstelveen     │\n# │ Ouder-Amstel   │\n# │ Amsterdam      │\n# │ Diemen         │\n# │ Waterland      │\n# └────────────────┘\n\nAnother, more scalable way to read Delta Lake tables with DuckDB is to use the Databricks Temporary Table Credentials API and the DuckDB Delta extension.",
    "crumbs": [
      "Setup",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>DuckDB on Databricks</span>"
    ]
  },
  {
    "objectID": "delta/GEOMETRY_vs_GEOGRAPHY.html",
    "href": "delta/GEOMETRY_vs_GEOGRAPHY.html",
    "title": "3  GEOMETRY vs GEOGRAPHY vs WKB",
    "section": "",
    "text": "If in a Delta Lake table you only store (lat, lon) values, use GEOGRAPHY (with the caveat that for st_distanceshpere and st_distancespheroid to work, you would need to cast geographies to geometry as geog::geometry(4326)).\nIf you use an SRID other than 4326 or multiple SRID’s, use GEOMETRY. If you need to calculate values in meters from lat/lon, you can still cast to GEOGRAPHY as needed and use measurement functions like st_area, st_length or st_perimeter.\nIf you need compatility with other tools, you might consider WKB – however, spark.write.parquet() would export GEOMETRY/GEOGRAPHY columns as WKB anyway.",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>GEOMETRY vs GEOGRAPHY vs WKB</span>"
    ]
  },
  {
    "objectID": "delta/GEOMETRY.html",
    "href": "delta/GEOMETRY.html",
    "title": "4  Storing spatial data in Delta Lake as GEOMETRY type",
    "section": "",
    "text": "4.1 Cleanup\nYou can store geometry or geography data in a Delta Lake table in a GEOMETRY column. See also GEOMETRY_vs_GEOGRAPHY.\n(Note that you could mix multiple SRID’s in one GEOMETRY column inside a Spark Dataframe, including inside a view, but must have a single SRID in a table.)\nLet’s calculate the distance in kilometers (on the surface of the spheroid Earth) between the two point examples above. st_distancespheroid expects lat/lon, so we’ll first convert everything to SRID 4326 – Null Island already is though.\nThe GEOGRAPHY type also offers further functions that take lon/lat as input and produce meters as output, such as st_area. So let’s calculate the size of the Bermuda Triangle in square kms:\n%sql\n-- drop table tmp_geometries",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Storing spatial data in Delta Lake as GEOMETRY type</span>"
    ]
  },
  {
    "objectID": "delta/GEOGRAPHY.html",
    "href": "delta/GEOGRAPHY.html",
    "title": "5  Storing spatial data in Delta Lake as GEOGRAPHY type",
    "section": "",
    "text": "5.1 Cleanup\nYou can store geography data in a Delta Lake table in a GEOGRAPHY column. See also GEOMETRY_vs_GEOGRAPHY.\nLet’s calculate the distance in kilometers (on the surface of the spheroid Earth) between the two point examples above. st_distancespheroid expects GEOMETRIES of lat/lon, so we’ll need to cast.\nThe GEOGRAPHY type also offers further functions that take lon/lat as input and produce meters as output, such as st_area. So let’s calculate the size of the Bermuda Triangle in square kms:\n%sql\n-- drop table tmp_geographies",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Storing spatial data in Delta Lake as GEOGRAPHY type</span>"
    ]
  },
  {
    "objectID": "delta/WKB.html",
    "href": "delta/WKB.html",
    "title": "6  Storing spatial data in Delta Lake as WKB",
    "section": "",
    "text": "6.1 Example usage with ST functions\nYou can store geometry or geography data in a Delta Lake table in a BINARY column as Well-known binary (WKB or EWKB). This is a more compact representation than well-known text (WKT), and widely supported incl. in the Geoparquet specification. On the other hand, unlike the newer GEOMETRY and GEOGRAPHY types, there is no higher level semantic support possible such as automatic spatial indexing (as of Aug 2025, coming soon). Also, you need to use the conversion function st_geomfromwkb or st_geomfromwekb before any other ST function.\nAnother example of Delta Lake tables with WKB columns are the Overture Maps datasets prepared by CARTO, available via the Databricks Marketplace. Follow the previous link to add any them (at no cost) to your catalog, if you haven’t yet. For example, for the below query, use Divisions (borders of countries and other administrative divisions):\nThese CARTO tables also show one pattern to organize and cluster tables with geometries: they include the bounding box columns __carto_xmin, __carto_xmax, __carto_ymin, ___carto_ymax and are clustered by these colums.\nAnother pattern would be to make use of spatial indexing such as H3.\nNOTE: as of 2025-08-03, ST functions are only available on DBR 17.1+ and thus not available on Databricks Free Edition, but this limitation might be lifted soon.\nBut what if you wanted to use\nshow srid\n%sql\n-- The distance between the UK and France\nwith countries as (\n  select\n    country,\n    st_geomfromwkb(geometry) wkb_geometry\n  from\n    carto_overture_maps_divisions.carto.division_area\n  where\n    subtype = 'country'\n    and class = 'land'\n),\nuk as (\n  select\n    wkb_geometry\n  from\n    countries\n  where\n    country = 'GB'\n),\nfr as (\n  select\n    wkb_geometry\n  from\n    countries\n  where\n    country = 'FR'\n)\nselect\n  st_distancespheroid(uk.wkb_geometry, fr.wkb_geometry) / 1e3 distance_km\nfrom\n  uk, fr\nNow the question is, if the Strait of Dover is about 32 kms narrow, why did we get more above? For this, we’d need to find out where exactly the shortest line is between the two countries. There is no st_shortestline yet in Databricks ST functions as of 2025-08-03, but we can use a Spark UDF with DuckDB spatial to fill this gap, see here TODO: .",
    "crumbs": [
      "Geospatial Delta Lake",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Storing spatial data in Delta Lake as WKB</span>"
    ]
  },
  {
    "objectID": "other_formats/import.html",
    "href": "other_formats/import.html",
    "title": "7  Import GeoPackage into Delta Lake with DuckDB",
    "section": "",
    "text": "Note\n\n\n\nNOTE: This example is focusing on using DuckDB to parse the GeoPackage. For more complex GeoPackages, you may need to install GDAL yourself and use the GDAL command line tools.\n\n\nThis example in facts works just as much for any spatial formats that DuckDB Spatial supports via its GDAL integration, see the output of ST_Drivers.\n\n%pip install duckdb --quiet\n\n\nimport duckdb\n\nduckdb.sql(\"install spatial; load spatial\")\n\n\nGPKG_URL = \"https://service.pdok.nl/kadaster/bestuurlijkegebieden/atom/v1_0/downloads/BestuurlijkeGebieden_2025.gpkg\"\n\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\n\n\nlayers = duckdb.sql(\n    f\"\"\"\nwith t as (\n    select unnest(layers) layer\n     from st_read_meta('{GPKG_URL}'))\nselect\n    layer.name layer_name,\n    layer.geometry_fields[1].name geom_field\nfrom t\"\"\"\n).df()\n\nlayers\n\n# Returns:\n\n# layer_name    geom_field\n# 0 gemeentegebied  geom\n# 1 landgebied  geom\n# 2 provinciegebied geom\n\n\n# pick a layer to read\nlayer_name, geom_field = layers.loc[0, [\"layer_name\", \"geom_field\"]]\n\nduckdb.sql(\n    f\"\"\"copy (\n  select * replace(st_aswkb({geom_field}) as {geom_field})\n  from\n    st_read(\n      '{GPKG_URL}',\n      layer='{layer_name}')\n  ) to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{layer_name}.parquet' (format parquet)\"\"\"\n)\n\n\nspark.read.parquet(\n    \"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{layer_name}.parquet\"\n).display()  # noqa: S108\n\nYou can store the above spark data frame as a Delta Lake table as needed.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import GeoPackage into Delta Lake with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html",
    "href": "other_formats/export.html",
    "title": "8  Export Delta Lake table to other formats with DuckDB",
    "section": "",
    "text": "8.1 Setup\n%pip install duckdb --quiet\n\nimport duckdb\n\nduckdb.sql(\"install spatial; load spatial\")\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\n\nGEOMETRY_COLUMN = \"geometry\"\n\nspark.sql(f\"create volume if not exists {CATALOG}.{SCHEMA}.{VOLUME}\")\nLet’s first create an example table with GEOMETRY columns:\n%sql\ncreate or replace table tmp_geometries as\nselect\n  st_point(0, 0, 4326) as geometry,\n  \"Null Island\" as name\nunion all\nselect\n  st_transform(st_point(155000, 463000, 28992), 4326) as geometry,\n  \"Onze Lieve Vrouwetoren\" as name\nunion all\nselect\n  st_makepolygon(\n    st_makeline(\n      array(\n        st_point(- 80.1935973, 25.7741566, 4326),\n        st_point(- 64.7563086, 32.3040273, 4326),\n        st_point(- 66.1166669, 18.4653003, 4326),\n        st_point(- 80.1935973, 25.7741566, 4326)\n      )\n    )\n  ) as geometry,\n  \"Bermuda Triangle\" as name;\n\nselect\n  *\nfrom\n  tmp_geometries\n-- Returns:\n\n-- _sqldf:pyspark.sql.connect.dataframe.DataFrame\n-- geometry:geometry(OGC:CRS84)\n-- name:string\n\n-- geometry name\n-- SRID=4326;POINT(0 0) Null Island\n-- SRID=4326;POINT(5.3872035084137675 52.15517230119224)    Onze Lieve Vrouwetoren\n-- SRID=4326;POLYGON((-80.1935973 25.7741566,-64.7563086 32.3040273,-66.1166669 18.4653003,-80.1935973 25.7741566)) Bermuda Triangle",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#parquet-files",
    "href": "other_formats/export.html#parquet-files",
    "title": "8  Export Delta Lake table to other formats with DuckDB",
    "section": "8.2 Parquet files",
    "text": "8.2 Parquet files\nWe’ll use DuckDB Spatial to write he Geoparquet file, so first, we output the above Delta Lake table as a directory of Parquet files, using lon/lat coordinates.\n(You could also use Databricks Temporary Table Credentials API to directly read the Delta Lake table with the DuckDB Delta Extension instead.)\n\nfrom pyspark.sql import functions as F\n\nspark.table(\"tmp_geometries\").withColumn(\n    \"geometry\", F.expr(\"st_transform(geometry, 4326)\")\n).write.mode(\"overwrite\").parquet(\n    f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet\"\n)\n\nWe will use the above parquet export as a stepping stone to produce other formats below.",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#geoparquet",
    "href": "other_formats/export.html#geoparquet",
    "title": "8  Export Delta Lake table to other formats with DuckDB",
    "section": "8.3 Geoparquet",
    "text": "8.3 Geoparquet\nWe can use duckdb to transform the Parquet files into a valid Geoparquet files:\n\n\n\n\n\n\nNote\n\n\n\n(Note that if you didn’t load the DuckDB Spatial extension, the below would still succeed but Geoparquet metadata would not be written.)\n\n\n\nquery = f\"\"\"\nload spatial;\ncopy (\nselect \n    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\nfrom\n    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n) to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries_geo.parquet' (format parquet)\"\"\"\nduckdb.sql(query)\n\nThere are more details around writing Geoparquet such as writing custom CRS’s or defining a “covering” using bounding boxes, but the above example is already a valid Geoparquet. For example, if your QGIS already supports the Parquet format (as of Aug 2025, the latest Windows version does but the latest macOS version doesn’t), then you can open this file in QGIS:\n\n\n\ngeoparquet in qgis\n\n\n(in fact, the GDAL Parquet reader used by QGIS can even open parquet files that are not valid geoparquet, as long as they have a WKB or WKT column and the column name and CRS matches the expected defaults or correctly defined)",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#flatgeobuf",
    "href": "other_formats/export.html#flatgeobuf",
    "title": "8  Export Delta Lake table to other formats with DuckDB",
    "section": "8.4 Flatgeobuf",
    "text": "8.4 Flatgeobuf\nExporting to Flatgeobuf is very similar to the above. Flatgeobuf as a format has two key advantages here: - It is faster to render (e.g. in QGIS) than Geoparquet, and - It can act as input to tippecanoe (see below), which we’ll use to produce PMTiles, which is even better suited for web mapping.\n\nquery = f\"\"\"\nload spatial;\ncopy (\nselect \n    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\nfrom\n    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n) to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb' (format flatgeobuf)\"\"\"\nduckdb.sql(query)",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "other_formats/export.html#cleanup",
    "href": "other_formats/export.html#cleanup",
    "title": "8  Export Delta Lake table to other formats with DuckDB",
    "section": "9.1 Cleanup",
    "text": "9.1 Cleanup\n\n%sql\ndrop table tmp_geometries",
    "crumbs": [
      "Import/Export to other formats",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Export Delta Lake table to other formats with DuckDB</span>"
    ]
  },
  {
    "objectID": "viz/native.html",
    "href": "viz/native.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "todo",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>native.html</span>"
    ]
  },
  {
    "objectID": "viz/lonboard.html",
    "href": "viz/lonboard.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "todo",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>lonboard.html</span>"
    ]
  },
  {
    "objectID": "viz/PMTiles.html",
    "href": "viz/PMTiles.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "todo",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>PMTiles.html</span>"
    ]
  },
  {
    "objectID": "viz/QGIS.html",
    "href": "viz/QGIS.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "todo",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>QGIS.html</span>"
    ]
  },
  {
    "objectID": "stfunctions/native.html",
    "href": "stfunctions/native.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "todo",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>native.html</span>"
    ]
  },
  {
    "objectID": "stfunctions/duckdb.html",
    "href": "stfunctions/duckdb.html",
    "title": "Databricks Free Edition Geospatial Cookbook with DuckDB",
    "section": "",
    "text": "todo",
    "crumbs": [
      "Spatial functions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>duckdb.html</span>"
    ]
  },
  {
    "objectID": "mapmatching/databricks_graphhopper.html",
    "href": "mapmatching/databricks_graphhopper.html",
    "title": "15  Run GraphHopper on Databricks",
    "section": "",
    "text": "Note that there is no particular advantage of running GraphHopper on Databricks, but if it is part of your larger workflow, then it is practical to keep this step in the same environment.\n\n# Replace the below parameters with the volume path you use.\n\nCATALOG = \"workspace\"\nSCHEMA = \"default\"\nVOLUME = \"default\"\nVOLDIR = \"graphhopper\"\n\nvolpath = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{VOLDIR}\"\n\nspark.sql(f\"create volume if not exists {CATALOG}.{SCHEMA}.{VOLUME}\")\n!mkdir -p /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{VOLDIR}\n\n\nimport os\n\nos.environ[\"VOLPATH\"] = volpath\n\n\n%sh\nrm -rf ${VOLPATH}\nmkdir ${VOLPATH}\ncd ${VOLPATH}\n\nwget -nv https://github.com/graphhopper/graphhopper/releases/download/8.0/graphhopper-web-8.0.jar\n\n# example config, trace, and matching pbf\nwget -nv https://github.com/graphhopper/graphhopper/raw/refs/tags/8.0/config-example.yml\nwget -nv https://raw.githubusercontent.com/graphhopper/graphhopper/refs/tags/8.0/web/src/test/resources/issue-13.gpx\nwget -nv https://download.geofabrik.de/europe/turkey-latest.osm.pbf\n\n#edit config to match the pbf\nsed -i \"s|datareader\\.file: \\\"\\\"|datareader.file: \\\"turkey-latest.osm.pbf\\\"|\" config-example.yml\n\n# note that this fails on many of the other gpx examples within the same folder, but that's a different problem.\njava -jar graphhopper-web-8.0.jar match --file config-example.yml --profile car issue-13.gpx\n\nVisualizing the generated GPX file with GPX Studio:\n\n\n\nissue-13 matched",
    "crumbs": [
      "Mapmatching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Run GraphHopper on Databricks</span>"
    ]
  },
  {
    "objectID": "appendix/https_install_duckdbextension.html",
    "href": "appendix/https_install_duckdbextension.html",
    "title": "16  Install DuckDB Extensions via HTTPS",
    "section": "",
    "text": "If your network blocks HTTP requests, you cannot simply install DuckDB extensions by e.g. INSTALL spatial; (it will fail after ~10 minutes), because of the dependency on httpfs, an extension itself – see details here.\nThe workaround is to separately download the httpfs extension (see here for the list of architectures, even though within Databricks, the below preset is correct):\n\n%pip install duckdb --quiet\n\nimport duckdb\nimport os\nfrom urllib.parse import urlparse\nimport requests\n\nARCHITECTURE = \"linux_amd64\"\nduckdb_version = duckdb.__version__\nurl = f\"https://extensions.duckdb.org/v{duckdb_version}/{ARCHITECTURE}/httpfs.duckdb_extension.gz\"\n\noutput_file = os.path.basename(urlparse(url).path)\nresponse = requests.get(url, timeout=30)\nresponse.raise_for_status()\nwith open(output_file, \"wb\") as f:\n    f.write(response.content)\n\nduckdb.install_extension(output_file)\n\nos.remove(output_file)\n\nduckdb.sql(\"SET custom_extension_repository='https://extensions.duckdb.org'\")\n\nAnd now you can install other extensions, such as:\n\nduckdb.sql(\"install spatial; load spatial\")",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Install DuckDB Extensions via HTTPS</span>"
    ]
  }
]