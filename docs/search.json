[
  {
    "objectID": "install_sf.html",
    "href": "install_sf.html",
    "title": "",
    "section": "",
    "text": "%sh\n# http traffic is blocked, so we need to use https for `apt` sources\nsed -i 's|http://|https://|g' /etc/apt/sources.list.d/ubuntu.sources\n\n# following https://r-spatial.github.io/sf/#ubuntu\nsudo apt -y update && apt install -y libudunits2-dev libgdal-dev libgeos-dev libproj-dev libsqlite3-dev\n\n\n%r\n# https://zacdav-db.github.io/dbrx-r-compendium/chapters/pkg-management/fast-installs.html#setting-repo-within-notebook\n\nrelease &lt;- system(\"lsb_release -c --short\", intern=T)\n\n# set the user agent string otherwise pre-compiled binarys aren't used\noptions(\n    HTTPUserAgent=sprintf(\n        \"R/%s R (%s)\",\n        getRversion(),\n        paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"]),\n    ),\n    repos=paste0(\"https://packagemanager.posit.co/cran/__linux__/\", release, \"/latest\"),\n)\n\n\n%r\ninstall.packages(\"sf\")\n\n\n%r\nlibrary(sf)\n\n\n%r\nst_point(c(1, 1))"
  },
  {
    "objectID": "viz/qgis_parquet.html",
    "href": "viz/qgis_parquet.html",
    "title": "",
    "section": "",
    "text": "The short answer is yes. You can export the table into Volumes into a parquet directory. If you use coalesce(1), then besides the metadata files there will only be one file named part-00000-*.parquet (where * stands for arbitrary other characters). If you download this file, with a bit of luck, you can open it in QGIS.\n%python\nTABLENAME = None  #FILL_IN: your table name\nCATALOG = None  #FILL_IN\nSCHEMA = None  #FILL_IN\nVOLUME = None  #FILL_IN\nVOLUME_PATH = None  #FILL_IN: path within the volume\n\nPARQUET_OUT = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{VOLUME_PATH.strip('/')}/{TABLENAME}.parquet\"\n\nspark.table(TABLENAME).coalesce(1).write.parquet(PARQUET_OUT)\nNow if you navigate in the Catalog Explorer to the Volume and the chosen volume path, you can download the file to your desktop.\nFurther details: QGIS uses GDAL to open your parquet file, and makes some assumptions, such as that your geometry is stored in WKB in a column called ‘geometry’, or it is WKT in a column including wkt in its name. You can further adjust this if needed in QGIS in the reading options parameter GEOM_POSSIBLE_NAMES. Similarly you can define or override the CRS if needed.\nIf the file you created takes too long to visualize, two things you can do are: - use ogr2ogr or DuckDB Spatial to write out a Flatgeobuf. It will be an even larger file, but will be rendered faster in QGIS. - Partition your parquet file by e.g. the H3 spatial index, and use one file per cell."
  },
  {
    "objectID": "viz/qgis_parquet.html#i-found-a-delta-table-in-the-unity-catalog-that-contains-geospatial-data-wkbwkt-column-can-i-download-it-to-my-computer-to-open-in-qgis",
    "href": "viz/qgis_parquet.html#i-found-a-delta-table-in-the-unity-catalog-that-contains-geospatial-data-wkbwkt-column-can-i-download-it-to-my-computer-to-open-in-qgis",
    "title": "",
    "section": "",
    "text": "The short answer is yes. You can export the table into Volumes into a parquet directory. If you use coalesce(1), then besides the metadata files there will only be one file named part-00000-*.parquet (where * stands for arbitrary other characters). If you download this file, with a bit of luck, you can open it in QGIS.\n%python\nTABLENAME = None  #FILL_IN: your table name\nCATALOG = None  #FILL_IN\nSCHEMA = None  #FILL_IN\nVOLUME = None  #FILL_IN\nVOLUME_PATH = None  #FILL_IN: path within the volume\n\nPARQUET_OUT = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{VOLUME_PATH.strip('/')}/{TABLENAME}.parquet\"\n\nspark.table(TABLENAME).coalesce(1).write.parquet(PARQUET_OUT)\nNow if you navigate in the Catalog Explorer to the Volume and the chosen volume path, you can download the file to your desktop.\nFurther details: QGIS uses GDAL to open your parquet file, and makes some assumptions, such as that your geometry is stored in WKB in a column called ‘geometry’, or it is WKT in a column including wkt in its name. You can further adjust this if needed in QGIS in the reading options parameter GEOM_POSSIBLE_NAMES. Similarly you can define or override the CRS if needed.\nIf the file you created takes too long to visualize, two things you can do are: - use ogr2ogr or DuckDB Spatial to write out a Flatgeobuf. It will be an even larger file, but will be rendered faster in QGIS. - Partition your parquet file by e.g. the H3 spatial index, and use one file per cell."
  },
  {
    "objectID": "viz/qgis_flatgeobuf.html",
    "href": "viz/qgis_flatgeobuf.html",
    "title": "",
    "section": "",
    "text": "%pip install duckdb --quiet\n\nimport duckdb\n\nduckdb.sql(\"\"\"install spatial; load spatial\"\"\")\n\n\nTABLE =  # FILL_IN\nCATALOG =   # FILL_IN\nSCHEMA =   # FILL_IN\nWKB_COLUMN =   # FILL_IN\nVOLUME =  # FILL_IN\n\nqgis_export_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{TABLE}\"\n\n!mkdir -p {qgis_export_path}\n\n\nspark.sql(f\"use {CATALOG}.{SCHEMA}\")\n\ndba = spark.table(TABLE).toArrow()\n\n\nduckdb.sql(f\"\"\"COPY (select\n  * replace(\n  st_geomfromwkb(\n    {WKB_COLUMN}\n  ) as {WKB_COLUMN}) \nfrom\n  dba) TO '{qgis_export_path}/{TABLE}.fgb' (\n    FORMAT GDAL,\n    DRIVER flatgeobuf,\n    LAYER_CREATION_OPTIONS 'TEMPORARY_DIR=/tmp/'\n);\n\"\"\")"
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html",
    "title": "DuckDB on Databricks",
    "section": "",
    "text": "DuckDB is a formidable new single-machine analytics tool, tracing its origins to the same Dutch research institute as Python. Crucially for this guide, it comes with a remarkably good Spatial extension.\nWhile Databricks comes with its own set of geospatial features, such as H3 functions, nothing stops you to use DuckDB on the side as well.\n(What you do have to keep in mind though is that while much of Databricks’s tooling, namely Apache Spark, is focused on big data analysis multi-node clusters, your DuckDB instead will just run on single-node, just like e.g. Pandas would. So use single-node clusters.)"
  },
  {
    "objectID": "duckdb-on-databricks/DuckDB_on_Databricks.html#running-duckdb-on-databricks",
    "href": "duckdb-on-databricks/DuckDB_on_Databricks.html#running-duckdb-on-databricks",
    "title": "DuckDB on Databricks",
    "section": "Running DuckDB on Databricks",
    "text": "Running DuckDB on Databricks\n\n%pip install duckdb --quiet\n\nimport duckdb\n\n# Install the Spatial Extension:\nduckdb.sql(\"install spatial; load spatial\")\n\nThis allows you to directly use the DuckDB Spatial, for example:\n\nduckdb.sql(\n    \"select st_distance(st_geomfromtext('POINT(0 0)'), st_geomfromtext('POINT(1 1)')) d\"\n)\n\n\nSpatial functions\n\n\nVisualize output\nIf your data is lon/lat, you can make use of the built-in point map visualization in Databricks Notebooks if you convert the DuckDB to a Spark DataFrame via Pandas. Once the result is shown, click on the + icon right of the Table tab to add the visualization “Map (Markers)” such as the one shown on the below image.\n\nquery = duckdb.sql(\n    \"\"\"\nwith t as (\n    select st_geomfromtext('POINT(0 0)') g\n    union all\n    select st_geomfromtext('POINT(1 1)') g\n)\nselect st_x(g) lon, st_y(g) lat from t\n\"\"\"\n)\nquery\n\n\nspark.createDataFrame(query.df()).display()\n\n\n\n\npoint_map\n\n\nOr visualize with lonboard, which will work also for other geometry types like linestrings and polygons:\n\n%pip install lonboard shapely --quiet\n\nfrom lonboard import viz\n\n\nquery = duckdb.sql(\n    \"\"\"\nselect st_geomfromtext('POINT(0 0)') g\nunion all\nselect st_geomfromtext('POINT(1 1)') g\n\"\"\"\n)\nquery\n\n\nviz(query).as_html()\n\n\n\nRead Delta Tables with DuckDB\nWe can read data from a delta table to duckdb via Arrow (or Pandas). (This assumes that the data volume is not prohibitively large to load into the memory of a single machine.)\n\ndfa = spark.read.table(tablename).toArrow()\n\nquery = duckdb.sql(\"\"\"\nselect\n    st_length(st_geomfromwkb(wkb)) length_m\nfrom\n    pda;\n\"\"\")\nquery\n\n\n\nWrite Delta Tables from DuckDB\nIf you want to write back a result to a delta table, you can use Pandas as an intermediary format:\n\nspark.createDataFrame(query.df()).createOrReplaceTempView(\"t\")\n# spark.createDataFrame(query.df()).write.saveAsTable(\"t\")\n\n\n%sql\nselect * from vw_t"
  },
  {
    "objectID": "delta2duckdb.html",
    "href": "delta2duckdb.html",
    "title": "Read in a Delta Lake table with a geometry column in DuckDB",
    "section": "",
    "text": "The best method depends on whether the table (or the sample you need) fits into (driver) memory or not. - If it does, you can simply go through Arrow. - If not, you can write out a copy of your data to plain Parquet file(s) in a Volume, which DuckDB can read. - Finally, if your you can use the Delta extension of DuckDB, but this somes with some limitations. Finally, if your data set is so large that you want to avoid the copy, you can use Temporary Table Credentials, but this requires extra permissions on the Unity Catalog object and the caller; furthermore, does not support GEOMETRY types yet."
  },
  {
    "objectID": "delta2duckdb.html#setup",
    "href": "delta2duckdb.html#setup",
    "title": "Read in a Delta Lake table with a geometry column in DuckDB",
    "section": "Setup",
    "text": "Setup\n\n%pip install duckdb --quiet\n\nimport duckdb\n\n\nCATALOG = \"workspace\"\nSCHEMA = \"dsparing\"\nVOLUME = \"default\"\nTABLENAME = \"tmp_delta2duck\"\n\ntable_fullname = f\"{CATALOG}.{SCHEMA}.{TABLENAME}\""
  },
  {
    "objectID": "delta2duckdb.html#delta-lake-to-duckdb-via-arrow",
    "href": "delta2duckdb.html#delta-lake-to-duckdb-via-arrow",
    "title": "Read in a Delta Lake table with a geometry column in DuckDB",
    "section": "Delta Lake to DuckDB via Arrow",
    "text": "Delta Lake to DuckDB via Arrow\nIf your (sample) data fits into memory, you can go through Arrow:\n\nspark.sql(\"select st_point(1, 2, 28992) as geometry\").write.mode(\n    \"overwrite\"\n).saveAsTable(table_fullname)\n\ndfa = spark.table(table_fullname).toArrow()\n\n\n[!NOTE] If the below install stalls, you might have HTTP traffic blocked, see [TODO: link] for the workaround.\n\n\nHTTP_BLOCKED = True\n\nif HTTP_BLOCKED:\n    import os\n    from urllib.parse import urlparse\n    import requests\n\n    ARCHITECTURE = \"linux_amd64\"\n    duckdb_version = duckdb.__version__\n    url = f\"https://extensions.duckdb.org/v{duckdb_version}/{ARCHITECTURE}/httpfs.duckdb_extension.gz\"\n\n    output_file = os.path.basename(urlparse(url).path)\n    response = requests.get(url, timeout=30)\n    response.raise_for_status()\n    with open(output_file, \"wb\") as f:\n        f.write(response.content)\n\n    duckdb.install_extension(output_file)\n\n    os.remove(output_file)\n\n    duckdb.sql(\"SET custom_extension_repository='https://extensions.duckdb.org'\")\n\n\nduckdb.sql(\"install spatial; load spatial\")\n\n\nduckdb.sql(\"select geometry.srid, st_geomfromwkb(geometry.wkb) geometry from dfa\")"
  },
  {
    "objectID": "delta2duckdb.html#delta-lake-to-duckdb-via-a-parquet-copy-in-volumes",
    "href": "delta2duckdb.html#delta-lake-to-duckdb-via-a-parquet-copy-in-volumes",
    "title": "Read in a Delta Lake table with a geometry column in DuckDB",
    "section": "Delta Lake to DuckDB via a Parquet copy in Volumes",
    "text": "Delta Lake to DuckDB via a Parquet copy in Volumes\nNote that the SRID is lost in this transformation, buth we can capture it anyway for later processing, e.g. for the Flatgeobuf export below.\n\nsrid = (\n    spark.table(table_fullname)\n    .selectExpr(\"any_value(st_srid(geometry)) as srid\")\n    .first()[0]\n)\n\n\nparquet_volume_path = (\n    f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/parquet/{TABLENAME}.parquet\"\n)\n\nspark.table(table_fullname).write.mode(\"overwrite\").parquet(parquet_volume_path)\n\n\n!ls {parquet_volume_path}/part-*.parquet\n\n\nduckdb.sql(\n    f\"\"\"select * replace(st_geomfromwkb(geometry) as geometry)\n    from read_parquet('{parquet_volume_path}/part-*.parquet')\"\"\"\n)\n\n\nSide story: Streaming Flatgeobuf\nWe can also use this Parquet copy and DuckDB to further convert it into a Flatgeobuf file, which can e.g. be very efficiently streamed to QGIS:\n\nfgb_volume_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/fgb/{TABLENAME}.fgb\"\n\nduckdb.sql(\n    f\"\"\"COPY (\n    select * replace(st_geomfromwkb(geometry) as geometry)\n    from read_parquet('{parquet_volume_path}/part-*.parquet')\n) TO '{fgb_volume_path}' (\n    FORMAT GDAL,\n    DRIVER flatgeobuf,\n    LAYER_CREATION_OPTIONS 'TEMPORARY_DIR=/tmp/',\n    SRS '{srid}'  -- doesn't seem to be used by QGIS downstream\n)\n\"\"\"\n)\n\nfgb_volume_path\n\nYou can download the above Flatgeobuf file and open it in QGIS – or even better, with a PAT, you can stream it via the Files API. Copy the result of the below cell into the source of your new vector layer in QGIS, replacing the section &lt;INSERT PAT&gt; with your actual PAT:\n\nf\"/vsicurl?header.Authorization=Bearer%20&lt;INSERT PAT&gt;&url=https://{spark.conf.get('spark.databricks.workspaceUrl')}/api/2.0/fs/files{fgb_volume_path}\""
  },
  {
    "objectID": "delta2duckdb.html#delta-lake-to-duckdb-via-temporary-table-credentials",
    "href": "delta2duckdb.html#delta-lake-to-duckdb-via-temporary-table-credentials",
    "title": "Read in a Delta Lake table with a geometry column in DuckDB",
    "section": "Delta Lake to DuckDB via Temporary Table Credentials",
    "text": "Delta Lake to DuckDB via Temporary Table Credentials\nThe delta extension of DuckDB does not support GEOMETRY types yet (as of July 2025), so the below approach only makes sense if your geometry column is still in WKB (or WKT).\n\nspark.sql(\n    f\"\"\"select * except (geometry), st_aswkb(geometry) as wkb_geometry\n    from {table_fullname}\"\"\"\n).write.mode(\"overwrite\").saveAsTable(f\"{table_fullname}_wkb\")\n\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.catalog import TableOperation\n\nw = WorkspaceClient()\n\n\nttc = w.temporary_table_credentials.generate_temporary_table_credentials(\n    operation=TableOperation.READ,\n    table_id=w.tables.get(f\"{table_fullname}_wkb\").table_id,\n)\n\nmetastore_region = w.metastores.get(w.metastores.current().metastore_id).region\n\nstorage_location = w.tables.get(f\"{table_fullname}_wkb\").storage_location\n\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = ttc.aws_temp_credentials.access_key_id\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = ttc.aws_temp_credentials.secret_access_key\nos.environ[\"AWS_SESSION_TOKEN\"] = ttc.aws_temp_credentials.session_token\nos.environ[\"AWS_DEFAULT_REGION\"] = metastore_region\n\n# These explicit installs is probably only needed if http is blocked, otherwise it would\n# be implicitly installed by the `CREATE SECRET` and `delta_scan()`\nduckdb.sql(\"install aws; load aws\")\nduckdb.sql(\"install delta; load delta\")\n\nduckdb.sql(\"\"\"\nCREATE OR REPLACE SECRET (\n    TYPE s3,\n    PROVIDER credential_chain\n)\"\"\")\n\n\nduckdb.sql(f\"\"\"\nselect \n* exclude (wkb_geometry), st_geomfromwkb(wkb_geometry) geometry\nfrom\ndelta_scan('{storage_location}')\n\"\"\")"
  },
  {
    "objectID": "mapmatching/databricks_graphhopper.html",
    "href": "mapmatching/databricks_graphhopper.html",
    "title": "",
    "section": "",
    "text": "CATALOG = \"spd_workspace\"\nSCHEMA = \"default\"\nVOLUME = \"spd_vol\"\nVOLDIR = \"graphhopper\"\n\nvolpath = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{VOLDIR}\"\n\n\nimport os\n\nos.environ[\"VOLPATH\"] = volpath\n\n\n!pwd\n\n\n!echo ${VOLPATH}\n\n\n%sh\nrm -rf ${VOLPATH}\nmkdir ${VOLPATH}\ncd ${VOLPATH}\n\nwget -nv https://github.com/graphhopper/graphhopper/releases/download/8.0/graphhopper-web-8.0.jar\n\n# example config, trace, and matching pbf\nwget -nv https://github.com/graphhopper/graphhopper/raw/refs/tags/8.0/config-example.yml\nwget -nv https://raw.githubusercontent.com/graphhopper/graphhopper/refs/tags/8.0/web/src/test/resources/issue-13.gpx\nwget -nv https://download.geofabrik.de/europe/turkey-latest.osm.pbf\n\n#edit config to match the pbf\nsed -i \"s|datareader\\.file: \\\"\\\"|datareader.file: \\\"turkey-latest.osm.pbf\\\"|\" config-example.yml\n\n# note that this fails on many of the other gpx examples within the same folder, but that's a different problem.\njava -jar graphhopper-web-8.0.jar match --file config-example.yml --profile car issue-13.gpx\n\n\n\n\nissue-13 matched"
  },
  {
    "objectID": "https_install_duckdbextension.html",
    "href": "https_install_duckdbextension.html",
    "title": "Install DuckDB Extensions via HTTPS",
    "section": "",
    "text": "If your network blocks HTTP requests, you cannot simply install DuckDB extensions by e.g. INSTALL spatial; (it will fail after ~10 minutes), because of the dependency on httpfs, an extension itself – see details here.\nThe workaround is to separately download the httpfs extension (see here for the list of architectures, even though within Databricks, the below preset is correct):\n\n%pip install duckdb --quiet\n\nimport duckdb\nimport os\nfrom urllib.parse import urlparse\nimport requests\n\nARCHITECTURE = \"linux_amd64\"\nduckdb_version = duckdb.__version__\nurl = f\"https://extensions.duckdb.org/v{duckdb_version}/{ARCHITECTURE}/httpfs.duckdb_extension.gz\"\n\noutput_file = os.path.basename(urlparse(url).path)\nresponse = requests.get(url, timeout=30)\nresponse.raise_for_status()\nwith open(output_file, \"wb\") as f:\n    f.write(response.content)\n\nduckdb.install_extension(output_file)\n\nos.remove(output_file)\n\nduckdb.sql(\"SET custom_extension_repository='https://extensions.duckdb.org'\")\n\nAnd now you can install other extensions, such as:\n\nduckdb.sql(\"install spatial; load spatial\")"
  },
  {
    "objectID": "stream_fgb.html",
    "href": "stream_fgb.html",
    "title": "Streaming Flatgeobuf",
    "section": "",
    "text": "%pip install duckdb --quiet\n\nimport duckdb\nimport os\n\n\nCATALOG = \"overturemaps\"\nSCHEMA = \"buildings\"\nVOLUME = \"default\"\nTABLENAME = \"building_nl\"\n\ntable_fullname = f\"{CATALOG}.{SCHEMA}.{TABLENAME}\"\n\n\nduckdb.sql(\"install spatial; load spatial\")"
  },
  {
    "objectID": "stream_fgb.html#setup",
    "href": "stream_fgb.html#setup",
    "title": "Streaming Flatgeobuf",
    "section": "",
    "text": "%pip install duckdb --quiet\n\nimport duckdb\nimport os\n\n\nCATALOG = \"overturemaps\"\nSCHEMA = \"buildings\"\nVOLUME = \"default\"\nTABLENAME = \"building_nl\"\n\ntable_fullname = f\"{CATALOG}.{SCHEMA}.{TABLENAME}\"\n\n\nduckdb.sql(\"install spatial; load spatial\")"
  },
  {
    "objectID": "stream_fgb.html#delta-lake-to-duckdb-via-temporary-table-credentials",
    "href": "stream_fgb.html#delta-lake-to-duckdb-via-temporary-table-credentials",
    "title": "Streaming Flatgeobuf",
    "section": "Delta Lake to DuckDB via Temporary Table Credentials",
    "text": "Delta Lake to DuckDB via Temporary Table Credentials\nThe delta extension of DuckDB does not support GEOMETRY types yet (as of July 2025), so the below approach only makes sense if your geometry column is still in WKB (or WKT).\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.catalog import TableOperation\n\nw = WorkspaceClient()\n\n\nttc = w.temporary_table_credentials.generate_temporary_table_credentials(\n    operation=TableOperation.READ,\n    table_id=w.tables.get(f\"{table_fullname}\").table_id,\n)\n\nmetastore_region = w.metastores.get(w.metastores.current().metastore_id).region\n\nstorage_location = w.tables.get(f\"{table_fullname}\").storage_location\n\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = ttc.aws_temp_credentials.access_key_id\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = ttc.aws_temp_credentials.secret_access_key\nos.environ[\"AWS_SESSION_TOKEN\"] = ttc.aws_temp_credentials.session_token\nos.environ[\"AWS_DEFAULT_REGION\"] = metastore_region\n\nduckdb.sql(\"\"\"\nCREATE OR REPLACE SECRET (\n    TYPE s3,\n    PROVIDER credential_chain\n)\"\"\")\n\n\nduckdb.sql(f\"\"\"\nselect * replace (st_geomfromwkb(geometry) as geometry)\nfrom\ndelta_scan('{storage_location}')\n\"\"\")\n\n\nSide story: Streaming Flatgeobuf\nWe can also use this Parquet copy and DuckDB to further convert it into a Flatgeobuf file, which can e.g. be very efficiently streamed to QGIS:\n\nfgb_volume_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/fgb/{TABLENAME}.fgb\"\n!mkdir -p \"$(dirname {fgb_volume_path})\"\n\nduckdb.sql(\n    f\"\"\"COPY (\n    select\n        * exclude(geometry, bbox, sources, names),  -- structs are not supported in fgb\n        st_geomfromwkb(geometry) as geometry,\n        bbox.*,\n        names.primary\n    from delta_scan('{storage_location}')\n    limit 100\n) TO '{fgb_volume_path}' (\n    FORMAT GDAL,\n    DRIVER flatgeobuf,\n    LAYER_CREATION_OPTIONS 'TEMPORARY_DIR=/tmp/'\n)\n\"\"\"\n)\n\nYou can download the above Flatgeobuf file and open it in QGIS – or even better, with a PAT, you can stream it via the Files API. Copy the result of the below cell into the source of your new vector layer in QGIS, replacing the section &lt;INSERT PAT&gt; with your actual PAT:\n\nprint(\n    f\"/vsicurl?header.Authorization=Bearer%20&lt;INSERT PAT&gt;&url=https://{spark.conf.get('spark.databricks.workspaceUrl')}/api/2.0/fs/files{fgb_volume_path}\"\n)"
  },
  {
    "objectID": "delta_geometry.html",
    "href": "delta_geometry.html",
    "title": "Create a Delta Lake table with a geometry column",
    "section": "",
    "text": "[!NOTE] We are using Spatial Functions and the GEOMETRY data type below, and as of 2025-07-16, you need to use Databricks Runtime 17.1+ for this. It is expected that this feature becomes available on Serverless Compute and DBSQL as well soon, see here.\n\n\n%sql\ncreate or replace table tmp_geometries as\nselect st_point(1, 2) as geometry_without_srid,\nst_point(155, 463, 28992) as geometry_with_srid\n;\n\nselect * from tmp_geometries\n\nOne geometry column can contain different geometry types such as point, linestring, polygon, or “multi-” types, but can only have one SRID:\n\n%sql\ncreate\nor replace table tmp_geometries as\nselect\n  \"Onze Lieve Vrouwetoren\" as name,\n  st_transform(st_point(155000, 463000, 28992), 4326) as geometry\nunion all\nselect\n  \"Bermuda Triangle\" as name,\n  st_makepolygon(\n    st_makeline(\n      array(\n        st_point(- 80.1935973, 25.7741566, 4326),\n        st_point(- 64.7563086, 32.3040273, 4326),\n        st_point(- 66.1166669, 18.4653003, 4326),\n        st_point(- 80.1935973, 25.7741566, 4326)\n      )\n    )\n  ) as geometry;\n\nselect\n  *\nfrom\n  tmp_geometries"
  },
  {
    "objectID": "viz/duckdb.html",
    "href": "viz/duckdb.html",
    "title": "",
    "section": "",
    "text": "[!NOTE] We are using Spatial Functions and the GEOMETRY data type below, and as of 2025-07-16, you need to use Databricks Runtime 17.1+ for this. It is expected that this feature becomes available on Serverless Compute and DBSQL as well soon, see here.\n\n\n%pip install duckdb lonboard shapely --quiet\nimport duckdb\nfrom lonboard import viz\n\n\nHTTP_BLOCKED = True\n\nif HTTP_BLOCKED:\n    import os\n    from urllib.parse import urlparse\n\n    import requests\n\n    ARCHITECTURE = \"linux_amd64\"\n    duckdb_version = duckdb.__version__\n    url = f\"https://extensions.duckdb.org/v{duckdb_version}/{ARCHITECTURE}/httpfs.duckdb_extension.gz\"\n\n    output_file = os.path.basename(urlparse(url).path)\n    response = requests.get(url, timeout=30)\n    response.raise_for_status()\n    with open(output_file, \"wb\") as f:\n        f.write(response.content)\n\n    duckdb.install_extension(output_file)\n\n    os.remove(output_file)\n\n    duckdb.sql(\"SET custom_extension_repository='https://extensions.duckdb.org'\")\n\n\nduckdb.sql(\"install spatial; load spatial\")\n\n\n%sql\ncreate\nor replace table tmp_geometries as\nselect\n  \"Onze Lieve Vrouwetoren\" as name,\n  st_transform(st_point(155000, 463000, 28992), 4326) as geometry\nunion all\nselect\n  \"Bermuda Triangle\" as name,\n  st_makepolygon(\n    st_makeline(\n      array(\n        st_point(- 80.1935973, 25.7741566, 4326),\n        st_point(- 64.7563086, 32.3040273, 4326),\n        st_point(- 66.1166669, 18.4653003, 4326),\n        st_point(- 80.1935973, 25.7741566, 4326)\n      )\n    )\n  ) as geometry;\n\nselect\n  *\nfrom\n  tmp_geometries\n\n\ndfa = spark.table(\"tmp_geometries\").limit(10_000).toArrow()\n\n(In the below example, if the SRID is 4326, then the st_transform is redundant.)\n\nquery = duckdb.sql(\"\"\"\nselect\n    * replace(st_transform(\n        st_geomfromwkb(geometry.wkb),\n        concat('EPSG:', geometry.srid),\n        'OGC:CRS84'\n    ) as geometry)\nfrom dfa\"\"\")\nquery\n\n\nviz(query).as_html()"
  },
  {
    "objectID": "viz/native_pointmap.html",
    "href": "viz/native_pointmap.html",
    "title": "Point Map Visualization",
    "section": "",
    "text": "Point map visualization is natively supported, if your table outputs longitude-latitude pairs. It is recommended to limit the query to not too many rows for performance reasons.\n\n[!NOTE] We are using Spatial Functions and the GEOMETRY data type below, and as of 2025-07-16, you need to use Databricks Runtime 17.1+ for this. It is expected that this feature becomes available on Serverless Compute and DBSQL as well soon, see here.\n\n\n%pip install geoarrow-pyarrow lonboard shapely --quiet\n\n\nfrom lonboard import viz\nimport geoarrow.pyarrow as ga\nimport pyarrow as pa\nimport pyproj\n\n\n%sql\ncreate\nor replace table tmp_geometries as\nselect\n  \"Onze Lieve Vrouwetoren\" as name,\n  st_transform(st_point(155000, 463000, 28992), 4326) as geometry\nunion all\nselect\n  \"Bermuda Triangle\" as name,\n  st_makepolygon(\n    st_makeline(\n      array(\n        st_point(- 80.1935973, 25.7741566, 4326),\n        st_point(- 64.7563086, 32.3040273, 4326),\n        st_point(- 66.1166669, 18.4653003, 4326),\n        st_point(- 80.1935973, 25.7741566, 4326)\n      )\n    )\n  ) as geometry;\n\nselect\n  *\nfrom\n  tmp_geometries\n\nOur example table contains not only points, but the native visualization can only show points, so one thing we can do is to show the centroids.\nIn the above example, the SRID happens to be 4326, therefore the below st_transform is a unnecessary. In general, however, you’d want to transform the geometries to 4326 in order to obtain valid (lon, lat) tuples.\n\n%sql\nselect\n  name,\n  st_transform(st_centroid(geometry), 4326) centroid,\n  st_x(centroid) lon,\n  st_y(centroid) lat\nfrom\n  tmp_geometries\n\n\ndfa = spark.table(\"tmp_geometries\").limit(10_000).toArrow()\n\n\n# Assuming that all SRID's within the table are the same, which they should be as the\n# SRID is part of the column type.\ncrs = pyproj.CRS.from_epsg(dfa[\"geometry\"][0][\"srid\"]).to_json()\n\n\nfield_chunks = [chunk.field(\"wkb\") for chunk in dfa[\"geometry\"].chunks]\ndfa_geoarrow = dfa.append_column(\n    \"geoarrow\", ga.with_crs(ga.as_geoarrow(pa.chunked_array(field_chunks)), crs)\n).drop(\"geometry\")\n\n\nviz(dfa_geoarrow).as_html()\n\nAlternatively, you can use DuckDB to visualize in lonboard, see the DuckDB tutorial for that."
  }
]
