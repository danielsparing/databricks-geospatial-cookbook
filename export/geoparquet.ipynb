{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9747d4b-9a95-48c1-9653-91434bf7966f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Export Delta Lake table to Geoparquet with DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f985e8b-d735-44bb-b766-4a2b28a1431d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c8e31d-b84f-4079-a883-34b10db781ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install duckdb --quiet\n",
    "\n",
    "import duckdb\n",
    "\n",
    "duckdb.sql(\"install spatial; load spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83b6e9d-9f4c-4004-a239-c24d14a64520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"default\"\n",
    "VOLUME = \"default\"\n",
    "\n",
    "GEOMETRY_COLUMN = \"geometry\"\n",
    "\n",
    "spark.sql(f\"create volume if not exists {CATALOG}.{SCHEMA}.{VOLUME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db787d74-c8a1-4ae4-9d44-78c4fd6d61f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's first create an example table with GEOMETRY columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48dd204-dc89-40d4-bd79-21a6ab2f568c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table tmp_geometries as\n",
    "select\n",
    "  st_point(0, 0, 4326) as geometry,\n",
    "  \"Null Island\" as name\n",
    "union all\n",
    "select\n",
    "  st_transform(st_point(155000, 463000, 28992), 4326) as geometry,\n",
    "  \"Onze Lieve Vrouwetoren\" as name\n",
    "union all\n",
    "select\n",
    "  st_makepolygon(\n",
    "    st_makeline(\n",
    "      array(\n",
    "        st_point(- 80.1935973, 25.7741566, 4326),\n",
    "        st_point(- 64.7563086, 32.3040273, 4326),\n",
    "        st_point(- 66.1166669, 18.4653003, 4326),\n",
    "        st_point(- 80.1935973, 25.7741566, 4326)\n",
    "      )\n",
    "    )\n",
    "  ) as geometry,\n",
    "  \"Bermuda Triangle\" as name;\n",
    "\n",
    "select\n",
    "  *\n",
    "from\n",
    "  tmp_geometries\n",
    "-- Returns:\n",
    "\n",
    "-- _sqldf:pyspark.sql.connect.dataframe.DataFrame\n",
    "-- geometry:geometry(OGC:CRS84)\n",
    "-- name:string\n",
    "\n",
    "-- geometry\tname\n",
    "-- SRID=4326;POINT(0 0)\tNull Island\n",
    "-- SRID=4326;POINT(5.3872035084137675 52.15517230119224)\tOnze Lieve Vrouwetoren\n",
    "-- SRID=4326;POLYGON((-80.1935973 25.7741566,-64.7563086 32.3040273,-66.1166669 18.4653003,-80.1935973 25.7741566))\tBermuda Triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e100357-be84-4bd7-9383-9d0821bcf9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We'll use DuckDB Spatial to write he Geoparquet file, so first, we output the above Delta Lake table as a directory of Parquet files, using lon/lat coordinates.\n",
    "\n",
    "(You could also use Databricks [Temporary Table Credentials API](https://docs.databricks.com/api/workspace/temporarytablecredentials) to directly read the Delta Lake table with the DuckDB [Delta Extension](https://duckdb.org/docs/stable/core_extensions/delta.html) instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03af0497-8c91-4a43-bd97-31975068d2b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.table(\"tmp_geometries\").withColumn(\n",
    "    \"geometry\", F.expr(\"st_transform(geometry, 4326)\")\n",
    ").write.mode(\"overwrite\").parquet(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4401283-30cc-4b5a-b546-02716bdff5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can use duckdb to transform the Parquet files into a valid Geoparquet files:\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "(Note that if you didn't load the DuckDB Spatial extension, the below would still succeed but Geoparquet metadata would _not_ be written.)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2147adf0-ae8f-423c-9546-2acbde40ada7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "load spatial;\n",
    "copy (\n",
    "select \n",
    "    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\n",
    "from\n",
    "    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n",
    ") to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries_geo.parquet' (format parquet)\"\"\"\n",
    "duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8c2e64-e456-462b-8158-96a4d4d94e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are more details around writing Geoparquet such as writing custom CRS's or defining a [\"covering\"](https://geoparquet.org/releases/v1.1.0/) using bounding boxes, but the above example is already a valid Geoparquet. For example, if your QGIS already supports the Parquet format (as of Aug 2025, the latest Windows version does but the latest macOS version doesn't), then you can open this file in QGIS:\n",
    "\n",
    "![geoparquet in qgis](img/geoparquet_qgis.png)\n",
    "\n",
    "(in fact, the [GDAL Parquet reader](https://gdal.org/en/stable/drivers/vector/parquet.html) used by QGIS can even open parquet files that are not valid geoparquet, as long as they have a WKB or WKT column and the column name and CRS matches the expected defaults or correctly defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aafd6bf0-2fef-4fa3-b334-b229a7b2b975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f430c3e5-1384-4150-920c-5969c26ecb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table tmp_geometries"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5649376984349439,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "geoparquet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
