{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b94c921-9477-474d-8ab3-6581c48da246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Read in a Delta Lake table with a geometry column in DuckDB\n",
    "\n",
    "The best method depends on whether the table (or the sample you need) fits into (driver) memory or not.\n",
    "- If it does, you can simply go through Arrow.\n",
    "- If not, you can write out a copy of your data to plain Parquet file(s) in a Volume, which DuckDB can read.\n",
    "- Finally, if your you can use the Delta extension of DuckDB, but this somes with some limitations. Finally, if your data set is so large that you want to avoid the copy, you can use Temporary Table Credentials, but this requires extra permissions on the Unity Catalog object and the caller; furthermore, does not support `GEOMETRY` types yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5212a74-c060-4f35-8875-95067863ac68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add0a825-6598-4b81-b78c-558c1ce82c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install duckdb --quiet\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9a34a2-cc0c-4830-9d83-9c2c5f0260ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"mainworkspace_1863054340605750\"\n",
    "SCHEMA = \"dsparing\"\n",
    "VOLUME = \"default\"\n",
    "TABLENAME = \"tmp_delta2duck\"\n",
    "\n",
    "table_fullname = f\"{CATALOG}.{SCHEMA}.{TABLENAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cb6e84-9556-4148-971b-a55f99132184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delta Lake to DuckDB via Arrow\n",
    "\n",
    "If your (sample) data fits into memory, you can go through Arrow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a921279-0f1e-4dba-8a76-ad6232c47a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select st_point(1, 2, 28992) as geometry\").write.mode(\n",
    "    \"overwrite\"\n",
    ").saveAsTable(table_fullname)\n",
    "\n",
    "dfa = spark.table(table_fullname).toArrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49de019e-d4d3-4c1e-8e76-de0ec6277afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> [!NOTE]\n",
    "> If the below install stalls, you might have HTTP traffic blocked, see [TODO: link] for the workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b4579461-1a25-4bc0-b974-9eb13ba9e8ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "HTTP_BLOCKED = True\n",
    "\n",
    "if HTTP_BLOCKED:\n",
    "    import os\n",
    "    from urllib.parse import urlparse\n",
    "    import requests\n",
    "\n",
    "    ARCHITECTURE = \"linux_amd64\"\n",
    "    duckdb_version = duckdb.__version__\n",
    "    url = f\"https://extensions.duckdb.org/v{duckdb_version}/{ARCHITECTURE}/httpfs.duckdb_extension.gz\"\n",
    "\n",
    "    output_file = os.path.basename(urlparse(url).path)\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    duckdb.install_extension(output_file)\n",
    "\n",
    "    os.remove(output_file)\n",
    "\n",
    "    duckdb.sql(\"SET custom_extension_repository='https://extensions.duckdb.org'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8be7b5c-f3e5-4ed9-bded-6d73d41b5004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duckdb.sql(\"install spatial; load spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fcce88-8f76-4258-9dde-9e29e3c19588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duckdb.sql(\"select geometry.srid, st_geomfromwkb(geometry.wkb) geometry from dfa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db1a3e6-9c5f-4066-b341-77c5f8473935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delta Lake to DuckDB via a Parquet copy in Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429915c6-a245-464a-a056-00eb7cee4b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that the SRID is lost in this transformation, buth we can capture it anyway for later processing, e.g. for the Flatgeobuf export below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3057a8c5-85c6-4f78-b5cf-8c3eb61c736a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srid = (\n",
    "    spark.table(table_fullname)\n",
    "    .selectExpr(\"any_value(st_srid(geometry)) as srid\")\n",
    "    .first()[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea54103-80f1-4b67-9210-d8b70f04b180",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752832197270}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_volume_path = (\n",
    "    f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/parquet/{TABLENAME}.parquet\"\n",
    ")\n",
    "\n",
    "spark.table(table_fullname).write.mode(\"overwrite\").parquet(parquet_volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222d47d4-9b99-49ea-abee-1176e1a55a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls {parquet_volume_path}/part-*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342e6b9d-38f7-489e-9e09-f25307f3facb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duckdb.sql(\n",
    "    f\"\"\"select * replace(st_geomfromwkb(geometry) as geometry)\n",
    "    from read_parquet('{parquet_volume_path}/part-*.parquet')\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4349afe6-bdca-4ff0-b5f9-7049432003cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Side story: Streaming Flatgeobuf\n",
    "We can also use this Parquet copy and DuckDB to further convert it into a Flatgeobuf file, which can e.g. be very efficiently streamed to QGIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a602da3c-4024-4906-982b-e86f70f0f03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fgb_volume_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/fgb/{TABLENAME}.fgb\"\n",
    "\n",
    "duckdb.sql(\n",
    "    f\"\"\"COPY (\n",
    "    select * replace(st_geomfromwkb(geometry) as geometry)\n",
    "    from read_parquet('{parquet_volume_path}/part-*.parquet')\n",
    ") TO '{fgb_volume_path}' (\n",
    "    FORMAT GDAL,\n",
    "    DRIVER flatgeobuf,\n",
    "    LAYER_CREATION_OPTIONS 'TEMPORARY_DIR=/tmp/',\n",
    "    SRS '{srid}'  -- doesn't seem to be used by QGIS downstream\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "fgb_volume_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c2122e5-da9f-4249-b6df-8dc8b751f595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can download the above Flatgeobuf file and open it in QGIS -- or even better, with a PAT, you can stream it via the Files API. Copy the result of the below cell into the source of your new vector layer in QGIS, replacing the section `<INSERT PAT>` with your actual PAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7facbcd2-dbbf-43a2-8282-cdbc9d3832a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f\"/vsicurl?header.Authorization=Bearer%20<INSERT PAT>&url=https://{spark.conf.get('spark.databricks.workspaceUrl')}/api/2.0/fs/files{fgb_volume_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23e577f-1f4b-4dfe-b6b5-077438096554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delta Lake to DuckDB via Temporary Table Credentials\n",
    "\n",
    "The `delta` extension of DuckDB does not support GEOMETRY types yet (as of July 2025), so the below approach only makes sense if your geometry column is still in WKB (or WKT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d2e3b2-3520-483f-884f-5c072b9bc313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"select * except (geometry), st_aswkb(geometry) as wkb_geometry\n",
    "    from {table_fullname}\"\"\"\n",
    ").write.mode(\"overwrite\").saveAsTable(f\"{table_fullname}_wkb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83572ae6-e8f0-4d2f-a6da-6f2b7b76b7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import TableOperation\n",
    "\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e409f2c2-337d-447e-810d-5795af6ea99a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ttc = w.temporary_table_credentials.generate_temporary_table_credentials(\n",
    "    operation=TableOperation.READ,\n",
    "    table_id=w.tables.get(f\"{table_fullname}_wkb\").table_id,\n",
    ")\n",
    "\n",
    "metastore_region = w.metastores.get(w.metastores.current().metastore_id).region\n",
    "\n",
    "storage_location = w.tables.get(f\"{table_fullname}_wkb\").storage_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5864f5-d02f-4428-b647-e68664b4ad97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = ttc.aws_temp_credentials.access_key_id\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = ttc.aws_temp_credentials.secret_access_key\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = ttc.aws_temp_credentials.session_token\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = metastore_region\n",
    "\n",
    "# These explicit installs is probably only needed if http is blocked, otherwise it would\n",
    "# be implicitly installed by the `CREATE SECRET` and `delta_scan()`\n",
    "duckdb.sql(\"install aws; load aws\")\n",
    "duckdb.sql(\"install delta; load delta\")\n",
    "\n",
    "duckdb.sql(\"\"\"\n",
    "CREATE OR REPLACE SECRET (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bfac81c-2388-4c27-80ba-e7e08b923e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duckdb.sql(f\"\"\"\n",
    "select \n",
    "* exclude (wkb_geometry), st_geomfromwkb(wkb_geometry) geometry\n",
    "from\n",
    "delta_scan('{storage_location}')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "173263f4-42b1-4d63-9677-1930955a3a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6287861021668267,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "delta2duckdb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
