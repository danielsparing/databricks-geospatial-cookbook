{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9747d4b-9a95-48c1-9653-91434bf7966f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Export Delta Lake table to other formats with DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f985e8b-d735-44bb-b766-4a2b28a1431d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c8e31d-b84f-4079-a883-34b10db781ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install duckdb --quiet\n",
    "\n",
    "import duckdb\n",
    "\n",
    "duckdb.sql(\"install spatial; load spatial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "If `install spatial` fails (especially if you are _not_ using the Free Edition or Serverless Compute, but classic compute), check whether HTTP is blocked on your (corporate) network. If so, then you need to work around it as described [here](../appendix/https_install_duckdbextension.ipynb).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83b6e9d-9f4c-4004-a239-c24d14a64520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"default\"\n",
    "VOLUME = \"default\"\n",
    "\n",
    "GEOMETRY_COLUMN = \"geometry\"\n",
    "\n",
    "spark.sql(f\"create volume if not exists {CATALOG}.{SCHEMA}.{VOLUME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db787d74-c8a1-4ae4-9d44-78c4fd6d61f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's first create an example table with GEOMETRY columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48dd204-dc89-40d4-bd79-21a6ab2f568c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table tmp_geometries as\n",
    "select\n",
    "  st_point(0, 0, 4326) as geometry,\n",
    "  \"Null Island\" as name\n",
    "union all\n",
    "select\n",
    "  st_transform(st_point(155000, 463000, 28992), 4326) as geometry,\n",
    "  \"Onze Lieve Vrouwetoren\" as name\n",
    "union all\n",
    "select\n",
    "  st_makepolygon(\n",
    "    st_makeline(\n",
    "      array(\n",
    "        st_point(- 80.1935973, 25.7741566, 4326),\n",
    "        st_point(- 64.7563086, 32.3040273, 4326),\n",
    "        st_point(- 66.1166669, 18.4653003, 4326),\n",
    "        st_point(- 80.1935973, 25.7741566, 4326)\n",
    "      )\n",
    "    )\n",
    "  ) as geometry,\n",
    "  \"Bermuda Triangle\" as name;\n",
    "\n",
    "select\n",
    "  *\n",
    "from\n",
    "  tmp_geometries\n",
    "-- Returns:\n",
    "\n",
    "-- _sqldf:pyspark.sql.connect.dataframe.DataFrame\n",
    "-- geometry:geometry(OGC:CRS84)\n",
    "-- name:string\n",
    "\n",
    "-- geometry\tname\n",
    "-- SRID=4326;POINT(0 0)\tNull Island\n",
    "-- SRID=4326;POINT(5.3872035084137675 52.15517230119224)\tOnze Lieve Vrouwetoren\n",
    "-- SRID=4326;POLYGON((-80.1935973 25.7741566,-64.7563086 32.3040273,-66.1166669 18.4653003,-80.1935973 25.7741566))\tBermuda Triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f304c11-5b07-489b-a509-3675486da463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e100357-be84-4bd7-9383-9d0821bcf9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We'll use DuckDB Spatial to write he Geoparquet file, so first, we output the above Delta Lake table as a directory of Parquet files, using lon/lat coordinates.\n",
    "\n",
    "(You could also use Databricks [Temporary Table Credentials API](https://docs.databricks.com/api/workspace/temporarytablecredentials) to directly read the Delta Lake table with the DuckDB [Delta Extension](https://duckdb.org/docs/stable/core_extensions/delta.html) instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03af0497-8c91-4a43-bd97-31975068d2b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.table(\"tmp_geometries\").withColumn(\n",
    "    \"geometry\", F.expr(\"st_transform(geometry, 4326)\")\n",
    ").write.mode(\"overwrite\").parquet(\n",
    "    f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f48f3d61-4f77-4545-88e5-77f51cc395c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will use the above parquet export as a stepping stone to produce other formats below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4401283-30cc-4b5a-b546-02716bdff5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Geoparquet\n",
    "\n",
    "We can use duckdb to transform the Parquet files into a valid [Geoparquet](https://geoparquet.org/) files:\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "(Note that if you didn't load the DuckDB Spatial extension, the below would still succeed but Geoparquet metadata would _not_ be written.)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2147adf0-ae8f-423c-9546-2acbde40ada7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "load spatial;\n",
    "copy (\n",
    "select \n",
    "    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\n",
    "from\n",
    "    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n",
    ") to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries_geo.parquet' (format parquet)\"\"\"\n",
    "duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8c2e64-e456-462b-8158-96a4d4d94e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are more details around writing Geoparquet such as writing custom CRS's or defining a [\"covering\"](https://geoparquet.org/releases/v1.1.0/) using bounding boxes, but the above example is already a valid Geoparquet. For example, if your QGIS already supports the Parquet format (as of Aug 2025, the latest Windows version does but the latest macOS version doesn't), then you can open this file in QGIS (after having downloaded from Volumes):\n",
    "\n",
    "![geoparquet in qgis](img/geoparquet_qgis.png)\n",
    "\n",
    "(in fact, the [GDAL Parquet reader](https://gdal.org/en/stable/drivers/vector/parquet.html) used by QGIS can even open parquet files that are not valid geoparquet, as long as they have a WKB or WKT column and the column name and CRS matches the expected defaults or correctly defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c2e1de-2302-40ad-8c8a-3780e480a03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Flatgeobuf\n",
    "\n",
    "Exporting to Flatgeobuf is very similar to the above. Flatgeobuf as a format has two key advantages here:\n",
    "- It is faster to render (e.g. in QGIS) than Geoparquet, and\n",
    "- It can act as input to `tippecanoe` (see below), which we'll use to produce PMTiles, which is even better suited for web mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fc227e-145a-4889-8612-1e10a053829b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "load spatial;\n",
    "copy (\n",
    "select \n",
    "    * replace (st_geomfromwkb({GEOMETRY_COLUMN}) as geometry)\n",
    "from\n",
    "    read_parquet('/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.parquet/part-*.parquet')\n",
    ") to '/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb'\n",
    "(FORMAT GDAL, DRIVER flatgeobuf, LAYER_CREATION_OPTIONS 'TEMPORARY_DIR=/tmp/')\"\"\"\n",
    "duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e0d007-7246-44ac-ab44-6dacebf23f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Streaming the Flatgeobuf file to QGIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "823121be-cbb6-4674-85f3-df35a00839eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can stream this to QGIS (i.e. without downloading the file first -- this is very useful for much larger datasets) via a token and the [Files API](https://docs.databricks.com/api/workspace/files/download). For example, after setting up a personal access token, you can stream the above file with a link like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19916277-513c-408a-a357-6abbd06d099b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f\"/vsicurl?header.Authorization=Bearer%20<YOUR_PERSONAL_ACCESS_TOKEN>&url=https://{spark.conf.get('spark.databricks.workspaceUrl')}/api/2.0/fs/files/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a32dd2d-31a5-4366-b37c-ee38f2197347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Replace `<YOUR_PERSONAL_ACCESS_TOPEN>` in the output of the above with your token, and you can copy the resulting string (together with \"/vsicurl\" at the beginning, but without the quotes) to QGIS, inserting a vector layer.\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "This way of streaming might work with other formats too, such as Parquet; however, for larger datasets, Flatgeobuf is a great choice. And for smaller datasets, simply downloading the file might be faster than setting up the above authentication.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76aa8f37-429e-4856-a9d7-a6ba93d43e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PMTiles\n",
    "\n",
    "For PMTiles, while theoretically we could keep using DuckDB Spatial with GDAL, we'll instead use [tippecanoe](https://github.com/felt/tippecanoe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b824448a-3f70-4500-9cbf-b646c2766f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# ~5 min\n",
    "cd /tmp && git clone https://github.com/felt/tippecanoe.git\n",
    "cd tippecanoe\n",
    "make -j\n",
    "make install PREFIX=$HOME/.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79a0a98-d41c-463c-a6ca-7c928fadad6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOME = os.environ[\"HOME\"]\n",
    "\n",
    "# see https://github.com/felt/tippecanoe/blob/main/README.md#try-this-first and e.g.\n",
    "# https://github.com/OvertureMaps/overture-tiles/blob/main/scripts/2024-07-22/places.sh\n",
    "# for possible options\n",
    "!{HOME}/.local/bin/tippecanoe -zg -rg -o /tmp/geometries.pmtiles --drop-densest-as-needed --extend-zooms-if-still-dropping --maximum-tile-bytes=2500000 --progress-interval=10 -l geometries --force /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.fgb\n",
    "# NOTE: this mv will emit an error related to updating metadata (\"mv: preserving\n",
    "# permissions for ‘[...]’: Operation not permitted\"), this can be ignored.\n",
    "!mv /tmp/geometries.pmtiles /Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/geometries.pmtiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "699bb546-c2f0-4763-a336-ce84d1e997b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To visualize, download the PMTiles from Volumes, and upload it to https://pmtiles.io/ (see below screenshot). To directly visualize it via Databricks Apps via downloading, see TODO:.\n",
    "\n",
    "![pmtiles_io](img/pmtilesio_geometries.png)\n",
    "\n",
    "TO be clear: the advantage the PMTiles format is to be able to visualize very large datasets such as all of OpenStreetMap -- this notebook only uses a very simple example but see TODO: for a larger case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aafd6bf0-2fef-4fa3-b334-b229a7b2b975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f430c3e5-1384-4150-920c-5969c26ecb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- drop table tmp_geometries"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8138020434231755,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "export",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
