{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6ae119a-d2fc-43b0-9736-0e777b9a8801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Scalable visualization with DuckDB Spatial MVT and MapLibreGL JS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2badbe03-0dbb-41c4-ac57-12510009e82b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[![Click on the image to play the video.](img/youtube_thumbnail.png)](https://www.youtube.com/watch?v=6d87gGNTyRg)\n",
    "\n",
    "(click on the above image to play the video.)\n",
    "\n",
    "![MapLibre visualization of a Delta Lake table with aGEOMETRY column, via DuckDB Spatial](img/maplibre.png)\n",
    "\n",
    "We have [MVT support](https://github.com/duckdb/duckdb-spatial/issues/241) in DuckDB Spatial since version 1.4. This means that we can feed MVT to e.g. MapLibreGL JS, as shown by DuckDB Spatial author Max Gabrielsson [here in an example Flask app](https://gist.github.com/Maxxen/37e4a9f8595ea5e6a20c0c8fbbefe955).\n",
    "\n",
    "(Another nice tool to consume DuckDB MVT's would be Martin, this is tracked in this [issue](https://github.com/maplibre/martin/issues/1693).)\n",
    "\n",
    "But how do we efficiently generate MVT's from a Delta Lake table containing a GEOMETRY, given the tile indices?\n",
    "\n",
    "The key thing to consider is that [now](https://www.databricks.com/blog/introducing-spatial-sql-databricks-80-functions-high-performance-geospatial-analytics) Databricks has very efficient spatial join filtering via e.g. `ST_Intersect`, especially if what you are filtering for is a constant. So the following query can be sub-second for e.g. a billion polygons such as Overture Maps buildings (note that we are not using any spatial grid or bounding box filters anymore):\n",
    "\n",
    "\n",
    "```sql\n",
    "select\n",
    "  geometry\n",
    "from\n",
    "  t\n",
    "where\n",
    "  st_intersects(\n",
    "    geometry,\n",
    "    st_geomfromtext(\n",
    "      'POLYGON ((5.44921875 52.160454557747045, 5.44921875 52.2143386082582, 5.537109374999999 52.2143386082582, 5.537109374999999 52.160454557747045, 5.44921875 52.160454557747045))'\n",
    "    )\n",
    "  )\n",
    "limit 30000\n",
    "```\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "For this to work fast, the geometry really has to be [GEOMETRY](https://docs.databricks.com/aws/en/sql/language-manual/data-types/geometry-type) (or probably GEOGRAPHY) type, and not WKB, which latter is still the case as of writing for the [CARTO-provided tables such as `carto_overture_maps_buildings.carto.building`](https://marketplace.databricks.com/details/ccacdfa3-b85d-4065-bd70-efa673c197e1/CARTO_Overture-Maps-Buildings).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45dcfa64-c3df-44dc-a842-57f5366d631d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a sample table\n",
    "\n",
    "We create here a sample table of buildings in the Netherlands -- the same worked for me also for all 2.5B Overture Maps buildings of the world, but if you tried to persist that table, you'd probably run against the daily usage limit of _Databricks Free Edition_ as I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f176cd45-e97f-4312-acb8-7d0b0e506c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OVERTUREMAPS_RELEASE = \"2025-10-22.0\"\n",
    "COUNTRY_CODE = \"NL\"\n",
    "\n",
    "country_bbox = (\n",
    "    spark.read.parquet(\n",
    "        f\"s3://overturemaps-us-west-2/release/{OVERTUREMAPS_RELEASE}/theme=divisions/type=division_area\"\n",
    "    )\n",
    "    .where(f\"subtype = 'country' and class = 'land' and country = '{COUNTRY_CODE}'\")\n",
    "    .select(\"bbox.*\")\n",
    "    .toPandas()\n",
    "    .iloc[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594b160d-8386-448b-b7a0-21e079c6aab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.read.parquet(\n",
    "    f\"s3://overturemaps-us-west-2/release/{OVERTUREMAPS_RELEASE}/theme=buildings/type=building\"\n",
    ").where(\n",
    "    f\"\"\"bbox.xmin < {country_bbox[\"xmax\"]}\n",
    "        and bbox.xmax > {country_bbox[\"xmin\"]}\n",
    "        and bbox.ymin < {country_bbox[\"ymax\"]}\n",
    "        and bbox.ymax > {country_bbox[\"ymin\"]}\n",
    "        \"\"\"\n",
    ").withColumn(\"geometry\", F.expr(\"st_geomfromwkb(geometry)\")).write.mode(\n",
    "    \"overwrite\"\n",
    ").saveAsTable(\"workspace.default.building_geom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a9a81a-1001-4d6c-a6af-472cbaf5bcba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can build on Maxxen's [gist](https://gist.github.com/Maxxen/37e4a9f8595ea5e6a20c0c8fbbefe955), with the following adjustments:\n",
    "\n",
    "- We keep DuckDB doing the MVT generation incl. the preprocessing of calculating the [ST_TileEnvelope](https://duckdb.org/docs/stable/core_extensions/spatial/functions#st_tileenvelope) for the tiles needed for the current viewport, but of course we need Databricks SQL to actually spatial filter our Delta Table (DuckDB delta_scan currently [does not read](https://github.com/duckdb/duckdb-delta/issues/248) GEOMETRY data types.)\n",
    "   - An alternative approach could be to [wrap the used DuckDB functions into Spark UDF's](../stfunctions/duckdb_udf.ipynb), if we wanted to move some compute from your browser to DBSQL.\n",
    "- For DBSQL we use the [Python `databricks-sql-connector`](https://docs.databricks.com/aws/en/dev-tools/python-sql-connector), authenticating with a [Personal Access Token](https://docs.databricks.com/aws/en/dev-tools/python-sql-connector#databricks-personal-access-token-authentication) -- for serious work, you'd want to use OAuth instead.\n",
    "- **Graceful feature limit.** What to do if a tile has too many features? A common solution would be to define a minimum zoom level, but this would make it very cumbersome to move around the map, so we define a `MAX_FEATURES_PER_TILE` instead. If this is reached, we gracefully fail and only show the tile boundaries -- the user would only need to further zoom in to reveal all the features within that viewport. (Of course you can throttle this value as you wish to find a balance between loading time and number of features shown.)\n",
    "- MVT expects SRID 3857, while our table is probably in another SRID, so we need to use some `st_transform` there and back.\n",
    "- **Tile throttling**. we also added JS code under `// === Tile throttling logic ===` to take a 2 second pause starting any zoom and move interaction, in order to avoid overloading the warehouse with tile requests and therefore avoid tile queueing.\n",
    "   - Note that in the current implementation this means that during zooming and moving the map, the feature layer is temporarily not visible -- this probably could be improved. For example, without tile throttling, the objects would remain visible during zoom/pan, but we would need to wait much longer for the results after a big move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "552c2528-4e0c-4628-af30-ac4c83f588cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the below video (showing a table with all 2.5B buildings worldwide, not just one country), you can see the tiling at work -- note how 1) the graceful feature limit means that \"too busy\" tiles are just shown as rectangles, and 2) zoom-and-pan pauses the feature layer but after a short timeout, the tiles are drawn, with sub-second latency per tile.\n",
    "\n",
    "Find the full code [here](https://github.com/danielsparing/databricks-geospatial-cookbook/blob/main/viz/dbgeo_duckdbmvt_app.py), which you can run [locally](https://flask.palletsprojects.com/en/stable/quickstart/) as a Flask app (you could also embed it within a Databricks App if preferred, but the local app is of course a bit more cost-effective).\n",
    "\n",
    "\n",
    "[![Click on the image to play the video.](img/maplibre_vid.png)](https://www.youtube.com/watch?v=6d87gGNTyRg)\n",
    "\n",
    "(click on the above image to play the video.)\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "What if you find this approach still too \"slow\", from the end-user standpoint? And/or, you find it \"cheating\" that we use `MAX_FEATURES_PER_TILE`? Then you can use [PMTiles](./PMTiles.ipynb). The difference is that with the MVT approach, you directly read the Delta Lake table, and the PMTile you would need to generate which means extra compute and time.\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "maplibregljs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
